{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Data Processing Pipeline\n",
        "\n",
        "**Purpose**: Prepare training data\n",
        "\n",
        "**Author**: Bai, Gengyuan\n",
        "\n",
        "**Tasks**:\n",
        "1. Process raw data (events.csv, item_properties) using PySpark\n",
        "2. Build candidate sets (prefix, item covisitation, category covisitation, popularity, user history)\n",
        "3. Generate all features required for training\n",
        "4. Save as parquet files for subsequent training\n",
        "\n",
        "**Data Windows**:\n",
        "- Training: 2015-05-01 to 2015-07-01 (2 months)\n",
        "- Validation: 2015-07-01 to 2015-08-01 (1 month)\n",
        "- Session gap: 30 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, lag, unix_timestamp, when, concat, lit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PySpark Data Processing Pipeline Start\n",
            "================================================================================\n",
            "Training window: [2015-05-01, 2015-07-01)\n",
            "Validation window: [2015-07-01, 2015-08-01)\n",
            "Session gap: 30 minutes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "DATA_DIR = Path(\"data/raw\")\n",
        "OUTPUT_DIR = Path(\"data/processed\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_START = \"2015-05-01\"\n",
        "TRAIN_END = \"2015-07-01\"\n",
        "VALID_START = \"2015-07-01\"\n",
        "VALID_END = \"2015-08-01\"\n",
        "SESSION_GAP_MINUTES = 30\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PySpark Data Processing Pipeline Start\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Training window: [{TRAIN_START}, {TRAIN_END})\")\n",
        "print(f\"Validation window: [{VALID_START}, {VALID_END})\")\n",
        "print(f\"Session gap: {SESSION_GAP_MINUTES} minutes\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 1: Initialize Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 1: Initializing Spark session...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/12/04 20:04:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/12/04 20:04:03 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Spark session created successfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 1: Initializing Spark session...\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Ecommerce_Training_Data_Preparation\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.local.dir\", \"/tmp/spark-temp\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(\"✓ Spark session created successfully\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 2: Load and Sessionize Events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 2: Loading events.csv and performing sessionization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2:=======>                                                   (1 + 7) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Events loaded: 1,902,445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"STEP 2: Loading events.csv and performing sessionization...\")\n",
        "\n",
        "events_path = f\"file://{(DATA_DIR / 'events.csv').absolute()}\"\n",
        "events_df = spark.read.csv(events_path, header=True, inferSchema=True)\n",
        "\n",
        "# Convert timestamp (from milliseconds to seconds)\n",
        "events_df = events_df.withColumn(\n",
        "    \"ts\", \n",
        "    F.from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Filter time range (includes both training and validation windows)\n",
        "events_df = events_df.filter(\n",
        "    (col(\"ts\") >= F.lit(TRAIN_START).cast(\"timestamp\")) &\n",
        "    (col(\"ts\") < F.lit(VALID_END).cast(\"timestamp\"))\n",
        ")\n",
        "\n",
        "# Rename columns\n",
        "events_df = events_df.select(\n",
        "    col(\"visitorid\").cast(\"bigint\").alias(\"user_id\"),\n",
        "    col(\"ts\"),\n",
        "    col(\"itemid\").cast(\"bigint\").alias(\"item_id\"),\n",
        "    col(\"event\")\n",
        ")\n",
        "\n",
        "print(f\"  Events loaded: {events_df.count():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9:===================================================>   (189 + 8) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sessions generated: 1,194,255\n",
            "✓ Sessionization completed\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Sessionization: 30-minute gap rule\n",
        "window_user_time = Window.partitionBy(\"user_id\").orderBy(\"ts\")\n",
        "\n",
        "events_df = events_df.withColumn(\n",
        "    \"prev_ts\",\n",
        "    lag(\"ts\").over(window_user_time)\n",
        ")\n",
        "\n",
        "events_df = events_df.withColumn(\n",
        "    \"time_gap_sec\",\n",
        "    when(\n",
        "        col(\"prev_ts\").isNotNull(),\n",
        "        unix_timestamp(\"ts\") - unix_timestamp(\"prev_ts\")\n",
        "    ).otherwise(0)\n",
        ")\n",
        "\n",
        "events_df = events_df.withColumn(\n",
        "    \"is_new_session\",\n",
        "    when(\n",
        "        (col(\"time_gap_sec\") > SESSION_GAP_MINUTES * 60) | col(\"prev_ts\").isNull(),\n",
        "        1\n",
        "    ).otherwise(0)\n",
        ")\n",
        "\n",
        "window_session = Window.partitionBy(\"user_id\").orderBy(\"ts\")\n",
        "events_df = events_df.withColumn(\n",
        "    \"session_num\",\n",
        "    F.sum(\"is_new_session\").over(window_session)\n",
        ")\n",
        "\n",
        "events_df = events_df.withColumn(\n",
        "    \"session_id\",\n",
        "    concat(col(\"user_id\").cast(\"string\"), lit(\"_\"), col(\"session_num\").cast(\"string\"))\n",
        ")\n",
        "\n",
        "# Clean intermediate columns\n",
        "events_df = events_df.select(\"session_id\", \"user_id\", \"ts\", \"item_id\", \"event\")\n",
        "\n",
        "# Cache to speed up subsequent operations\n",
        "events_df.cache()\n",
        "\n",
        "session_count = events_df.select(\"session_id\").distinct().count()\n",
        "print(f\"  Sessions generated: {session_count:,}\")\n",
        "print(\"✓ Sessionization completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 3: Load Item Category Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 3: Loading item properties and extracting category...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 21:===================================>                    (10 + 6) / 16]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Items with category: 417,053\n",
            "✓ Category information loaded\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"STEP 3: Loading item properties and extracting category...\")\n",
        "\n",
        "part1_path = f\"file://{(DATA_DIR / 'item_properties_part1.csv').absolute()}\"\n",
        "part2_path = f\"file://{(DATA_DIR / 'item_properties_part2.csv').absolute()}\"\n",
        "\n",
        "props1 = spark.read.csv(part1_path, header=True, inferSchema=True)\n",
        "props2 = spark.read.csv(part2_path, header=True, inferSchema=True)\n",
        "\n",
        "# Merge two parts\n",
        "item_props = props1.union(props2)\n",
        "\n",
        "# Convert timestamp\n",
        "item_props = item_props.withColumn(\n",
        "    \"ts\",\n",
        "    F.from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Keep only categoryid property, and take the latest value\n",
        "item_props = item_props.filter(col(\"property\") == \"categoryid\")\n",
        "\n",
        "item_props = item_props.select(\n",
        "    col(\"itemid\").cast(\"bigint\").alias(\"item_id\"),\n",
        "    col(\"value\").cast(\"bigint\").alias(\"category_id\"),\n",
        "    col(\"ts\")\n",
        ")\n",
        "\n",
        "# For each item, take the latest category_id\n",
        "window_item = Window.partitionBy(\"item_id\").orderBy(F.desc(\"ts\"))\n",
        "item_props = item_props.withColumn(\"rn\", F.row_number().over(window_item))\n",
        "item_category = item_props.filter(col(\"rn\") == 1).select(\"item_id\", \"category_id\")\n",
        "\n",
        "item_category.cache()\n",
        "print(f\"  Items with category: {item_category.count():,}\")\n",
        "print(\"✓ Category information loaded\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 4: Extract Add-to-Cart Events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 4: Extracting add-to-cart events...\n",
            "  Training ATC events: 29,244\n",
            "  Validation ATC events: 17,151\n",
            "✓ ATC event extraction completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 4: Extracting add-to-cart events...\")\n",
        "\n",
        "# Join events with category\n",
        "atc_events = events_df.filter(col(\"event\") == \"addtocart\") \\\n",
        "    .join(item_category, \"item_id\", \"inner\") \\\n",
        "    .select(\n",
        "        col(\"session_id\"),\n",
        "        col(\"user_id\"),\n",
        "        col(\"ts\").alias(\"atc_ts\"),\n",
        "        col(\"item_id\"),\n",
        "        col(\"category_id\")\n",
        "    )\n",
        "\n",
        "atc_events.cache()\n",
        "\n",
        "# Split into training and validation sets\n",
        "atc_train = atc_events.filter(\n",
        "    (col(\"atc_ts\") >= F.lit(TRAIN_START).cast(\"timestamp\")) &\n",
        "    (col(\"atc_ts\") < F.lit(TRAIN_END).cast(\"timestamp\"))\n",
        ")\n",
        "\n",
        "atc_valid = atc_events.filter(\n",
        "    (col(\"atc_ts\") >= F.lit(VALID_START).cast(\"timestamp\")) &\n",
        "    (col(\"atc_ts\") < F.lit(VALID_END).cast(\"timestamp\"))\n",
        ")\n",
        "\n",
        "atc_train.cache()\n",
        "atc_valid.cache()\n",
        "\n",
        "n_atc_train = atc_train.count()\n",
        "n_atc_valid = atc_valid.count()\n",
        "\n",
        "print(f\"  Training ATC events: {n_atc_train:,}\")\n",
        "print(f\"  Validation ATC events: {n_atc_valid:,}\")\n",
        "print(\"✓ ATC event extraction completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 5: Build Candidate Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 5: Building candidate sets...\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 5: Building candidate sets...\")\n",
        "\n",
        "def build_candidates_spark(atc_df, split_name, train_cutoff_str):\n",
        "    \"\"\"\n",
        "    Build candidate category sets for given ATC events\n",
        "    Includes: prefix, item covisitation, category covisitation, popularity, user history\n",
        "    \"\"\"\n",
        "    print(f\"  Building {split_name} candidates...\")\n",
        "    \n",
        "    train_cutoff = F.lit(train_cutoff_str).cast(\"timestamp\")\n",
        "    \n",
        "    # 1. Prefix candidates: all categories in session prefix\n",
        "    prefix_cands = atc_df.alias(\"a\") \\\n",
        "        .join(\n",
        "            events_df.alias(\"se\"),\n",
        "            (col(\"a.session_id\") == col(\"se.session_id\")) & (col(\"se.ts\") < col(\"a.atc_ts\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .select(\n",
        "            col(\"a.session_id\"),\n",
        "            col(\"a.atc_ts\"),\n",
        "            col(\"ic.category_id\").alias(\"category_id\")\n",
        "        ).distinct()\n",
        "    \n",
        "    # 2. Item Co-visitation candidates\n",
        "    # Calculate item-item co-occurrence (before training cutoff)\n",
        "    train_events = events_df.filter(col(\"ts\") < train_cutoff)\n",
        "    \n",
        "    item_covis = train_events.alias(\"a\") \\\n",
        "        .join(\n",
        "            train_events.alias(\"b\"),\n",
        "            (col(\"a.session_id\") == col(\"b.session_id\")) & (col(\"a.item_id\") < col(\"b.item_id\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .groupBy(col(\"a.item_id\").alias(\"item_a\"), col(\"b.item_id\").alias(\"item_b\")) \\\n",
        "        .agg(F.count(\"*\").alias(\"covis\")) \\\n",
        "        .filter(col(\"covis\") >= 3)\n",
        "    \n",
        "    # For each ATC's prefix items, find co-occurring items and convert to categories\n",
        "    prefix_items = atc_df.alias(\"a\") \\\n",
        "        .join(\n",
        "            events_df.alias(\"se\"),\n",
        "            (col(\"a.session_id\") == col(\"se.session_id\")) & (col(\"se.ts\") < col(\"a.atc_ts\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .select(\n",
        "            col(\"a.session_id\"),\n",
        "            col(\"a.atc_ts\"),\n",
        "            col(\"se.item_id\")\n",
        "        )\n",
        "    \n",
        "    itemcovis_cands = prefix_items.alias(\"pi\") \\\n",
        "        .join(item_covis.alias(\"iv\"), col(\"pi.item_id\") == col(\"iv.item_a\"), \"inner\") \\\n",
        "        .join(item_category.alias(\"ic2\"), col(\"iv.item_b\") == col(\"ic2.item_id\"), \"inner\") \\\n",
        "        .groupBy(col(\"pi.session_id\"), col(\"pi.atc_ts\"), col(\"ic2.category_id\")) \\\n",
        "        .agg(F.max(\"iv.covis\").alias(\"max_covis\")) \\\n",
        "        .withColumn(\n",
        "            \"rn\",\n",
        "            F.row_number().over(\n",
        "                Window.partitionBy(\"session_id\", \"atc_ts\").orderBy(F.desc(\"max_covis\"))\n",
        "            )\n",
        "        ) \\\n",
        "        .filter(col(\"rn\") <= 15) \\\n",
        "        .select(col(\"session_id\"), col(\"atc_ts\"), col(\"ic2.category_id\").alias(\"category_id\"))\n",
        "    \n",
        "    # 3. Category Co-visitation candidates\n",
        "    # Calculate category-category co-occurrence\n",
        "    train_events_with_cat = train_events.alias(\"se\") \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .select(col(\"se.session_id\"), col(\"ic.category_id\"))\n",
        "    \n",
        "    cat_covis = train_events_with_cat.alias(\"a\") \\\n",
        "        .join(\n",
        "            train_events_with_cat.alias(\"b\"),\n",
        "            (col(\"a.session_id\") == col(\"b.session_id\")) & (col(\"a.category_id\") < col(\"b.category_id\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .groupBy(col(\"a.category_id\").alias(\"cat_a\"), col(\"b.category_id\").alias(\"cat_b\")) \\\n",
        "        .agg(F.countDistinct(\"a.session_id\").alias(\"cooccur\")) \\\n",
        "        .filter(col(\"cooccur\") >= 5)\n",
        "    \n",
        "    prefix_cats = atc_df.alias(\"a\") \\\n",
        "        .join(\n",
        "            events_df.alias(\"se\"),\n",
        "            (col(\"a.session_id\") == col(\"se.session_id\")) & (col(\"se.ts\") < col(\"a.atc_ts\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .select(\n",
        "            col(\"a.session_id\"),\n",
        "            col(\"a.atc_ts\"),\n",
        "            col(\"ic.category_id\")\n",
        "        )\n",
        "    \n",
        "    catcovis_cands = prefix_cats.alias(\"pc\") \\\n",
        "        .join(cat_covis.alias(\"cc\"), col(\"pc.category_id\") == col(\"cc.cat_a\"), \"inner\") \\\n",
        "        .groupBy(col(\"pc.session_id\"), col(\"pc.atc_ts\"), col(\"cc.cat_b\")) \\\n",
        "        .agg(F.max(\"cc.cooccur\").alias(\"max_cooccur\")) \\\n",
        "        .withColumn(\n",
        "            \"rn\",\n",
        "            F.row_number().over(\n",
        "                Window.partitionBy(\"session_id\", \"atc_ts\").orderBy(F.desc(\"max_cooccur\"))\n",
        "            )\n",
        "        ) \\\n",
        "        .filter(col(\"rn\") <= 10) \\\n",
        "        .select(col(\"session_id\"), col(\"atc_ts\"), col(\"cat_b\").alias(\"category_id\"))\n",
        "    \n",
        "    # 4. Popularity candidates: top 20 globally most popular categories\n",
        "    cat_pop = train_events.alias(\"se\") \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .groupBy(\"ic.category_id\") \\\n",
        "        .agg(F.count(\"*\").alias(\"cnt\")) \\\n",
        "        .orderBy(F.desc(\"cnt\")) \\\n",
        "        .limit(20)\n",
        "    \n",
        "    pop_cands = atc_df.alias(\"a\").crossJoin(cat_pop.select(col(\"category_id\").alias(\"pop_cat_id\"))) \\\n",
        "        .select(col(\"a.session_id\"), col(\"a.atc_ts\"), col(\"pop_cat_id\").alias(\"category_id\"))\n",
        "    \n",
        "    # 5. User History candidates: user's historically viewed categories (recent 10)\n",
        "    user_past_cats = train_events.alias(\"se\") \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .filter(col(\"se.ts\") < train_cutoff) \\\n",
        "        .groupBy(col(\"se.user_id\"), col(\"ic.category_id\")) \\\n",
        "        .agg(F.max(\"se.ts\").alias(\"last_seen\"))\n",
        "    \n",
        "    userhist_cands = atc_df.alias(\"a\") \\\n",
        "        .join(\n",
        "            user_past_cats.alias(\"upc\"),\n",
        "            (col(\"a.user_id\") == col(\"upc.user_id\")) & (col(\"upc.last_seen\") < col(\"a.atc_ts\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .withColumn(\n",
        "            \"rn\",\n",
        "            F.row_number().over(\n",
        "                Window.partitionBy(\"a.session_id\", \"a.atc_ts\").orderBy(F.desc(\"upc.last_seen\"))\n",
        "            )\n",
        "        ) \\\n",
        "        .filter(col(\"rn\") <= 10) \\\n",
        "        .select(col(\"a.session_id\"), col(\"a.atc_ts\"), col(\"upc.category_id\").alias(\"category_id\"))\n",
        "    \n",
        "    # Merge all candidates\n",
        "    all_candidates = prefix_cands \\\n",
        "        .union(itemcovis_cands) \\\n",
        "        .union(catcovis_cands) \\\n",
        "        .union(pop_cands) \\\n",
        "        .union(userhist_cands) \\\n",
        "        .distinct()\n",
        "    \n",
        "    n_cands = all_candidates.count()\n",
        "    print(f\"    {split_name}: {n_cands:,} candidates\")\n",
        "    \n",
        "    return all_candidates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Building train candidates...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    train: 948,580 candidates\n",
            "  Building valid candidates...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 222:===================================================> (203 + 5) / 208]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    valid: 538,331 candidates\n",
            "✓ Candidate set building completed\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Build training and validation candidate sets\n",
        "train_candidates = build_candidates_spark(atc_train, \"train\", TRAIN_END)\n",
        "valid_candidates = build_candidates_spark(atc_valid, \"valid\", TRAIN_END)\n",
        "\n",
        "train_candidates.cache()\n",
        "valid_candidates.cache()\n",
        "\n",
        "print(\"✓ Candidate set building completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 5.5: Training Category Embeddings (Word2Vec)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Extracted 160,240 category sequences for training\n",
            "  Trained embeddings for 921 categories\n",
            "  Example: Category 1051 most similar categories: [(218, 0.83), (1192, 0.822), (1213, 0.803), (626, 0.796), (1375, 0.753)]\n",
            "✓ Word2Vec training completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 5.5: Training Category Embeddings (Word2Vec)...\")\n",
        "\n",
        "# Extract category sequences for each session (for Word2Vec training)\n",
        "cat_seqs_spark = events_df.filter(col(\"ts\") < F.lit(TRAIN_END).cast(\"timestamp\")) \\\n",
        "    .join(item_category, \"item_id\", \"inner\") \\\n",
        "    .select(\"session_id\", \"ts\", \"category_id\") \\\n",
        "    .orderBy(\"session_id\", \"ts\")\n",
        "\n",
        "cat_seqs_spark = cat_seqs_spark.groupBy(\"session_id\").agg(\n",
        "    F.collect_list(\"category_id\").alias(\"cat_sequence\")\n",
        ")\n",
        "\n",
        "# Convert to Pandas to use gensim\n",
        "cat_seqs_pd = cat_seqs_spark.toPandas()\n",
        "\n",
        "# Prepare training data (convert to string lists)\n",
        "sequences = [[str(cat) for cat in seq if cat is not None] for seq in cat_seqs_pd['cat_sequence']]\n",
        "sequences = [seq for seq in sequences if len(seq) >= 2]  # Filter short sequences\n",
        "\n",
        "print(f\"  Extracted {len(sequences):,} category sequences for training\")\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=sequences,\n",
        "    vector_size=16,\n",
        "    window=5,\n",
        "    min_count=3,\n",
        "    workers=4,\n",
        "    sg=1,\n",
        "    epochs=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"  Trained embeddings for {len(w2v_model.wv)} categories\")\n",
        "\n",
        "# Create embedding lookup dictionary\n",
        "cat_embeddings = {int(cat): w2v_model.wv[cat] for cat in w2v_model.wv.index_to_key}\n",
        "\n",
        "# Show similarity check\n",
        "sample_cat = list(cat_embeddings.keys())[0]\n",
        "similar = w2v_model.wv.most_similar(str(sample_cat), topn=5)\n",
        "print(f\"  Example: Category {sample_cat} most similar categories: {[(int(c), round(s, 3)) for c, s in similar]}\")\n",
        "\n",
        "print(\"✓ Word2Vec training completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 6: Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 6: Feature engineering...\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 6: Feature engineering...\")\n",
        "\n",
        "def build_features_spark(atc_df, candidates_df, split_name, train_cutoff_str):\n",
        "    \"\"\"\n",
        "    Build all features required for training for the candidate set\n",
        "    \"\"\"\n",
        "    print(f\"  Building {split_name} features...\")\n",
        "    \n",
        "    train_cutoff = F.lit(train_cutoff_str).cast(\"timestamp\")\n",
        "    train_events = events_df.filter(col(\"ts\") < train_cutoff)\n",
        "    \n",
        "    # Join ATC with candidates\n",
        "    base = atc_df.alias(\"a\") \\\n",
        "        .join(\n",
        "            candidates_df.alias(\"c\"),\n",
        "            (col(\"a.session_id\") == col(\"c.session_id\")) & (col(\"a.atc_ts\") == col(\"c.atc_ts\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .select(\n",
        "            col(\"a.session_id\"),\n",
        "            col(\"a.user_id\"),\n",
        "            col(\"a.atc_ts\"),\n",
        "            col(\"a.category_id\").alias(\"true_category_id\"),\n",
        "            col(\"c.category_id\").alias(\"cand_category_id\")\n",
        "        )\n",
        "    \n",
        "    # 1. Prefix statistics features\n",
        "    prefix_events = base.alias(\"b\") \\\n",
        "        .join(\n",
        "            events_df.alias(\"se\"),\n",
        "            (col(\"b.session_id\") == col(\"se.session_id\")) & (col(\"se.ts\") < col(\"b.atc_ts\")),\n",
        "            \"left\"\n",
        "        ) \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"left\")\n",
        "    \n",
        "    prefix_stats = prefix_events.groupBy(\n",
        "        col(\"b.session_id\"), col(\"b.atc_ts\"), col(\"b.cand_category_id\")\n",
        "    ).agg(\n",
        "        F.countDistinct(col(\"se.item_id\")).alias(\"n_prefix_items\"),\n",
        "        F.count(col(\"se.item_id\")).alias(\"n_prefix_events\"),\n",
        "        F.sum(when(col(\"ic.category_id\") == col(\"b.cand_category_id\"), 1).otherwise(0)).alias(\"cat_count_in_prefix\"),\n",
        "        F.max(\n",
        "            when(col(\"ic.category_id\") == col(\"b.cand_category_id\"), \n",
        "                 unix_timestamp(col(\"b.atc_ts\")) - unix_timestamp(col(\"se.ts\")))\n",
        "        ).alias(\"recency_sec\"),\n",
        "        F.min(col(\"se.ts\")).alias(\"session_start\"),\n",
        "        F.countDistinct(col(\"ic.category_id\")).alias(\"n_unique_cats_in_session\")\n",
        "    ).select(\n",
        "        col(\"session_id\"),\n",
        "        col(\"atc_ts\"),\n",
        "        col(\"cand_category_id\"),\n",
        "        col(\"n_prefix_items\"),\n",
        "        col(\"n_prefix_events\"),\n",
        "        col(\"cat_count_in_prefix\"),\n",
        "        col(\"recency_sec\"),\n",
        "        col(\"session_start\"),\n",
        "        col(\"n_unique_cats_in_session\")\n",
        "    )\n",
        "    \n",
        "    # 2. Category global popularity\n",
        "    cat_pop = train_events.alias(\"se\") \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .groupBy(col(\"ic.category_id\")) \\\n",
        "        .agg(F.count(\"*\").alias(\"global_pop\")) \\\n",
        "        .select(\n",
        "            col(\"category_id\"),\n",
        "            col(\"global_pop\")\n",
        "        )\n",
        "    \n",
        "    # 3. User-Category affinity\n",
        "    user_cat_aff = train_events.alias(\"se\") \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .groupBy(col(\"se.user_id\"), col(\"ic.category_id\")) \\\n",
        "        .agg(\n",
        "            F.count(\"*\").alias(\"user_cat_interactions\"),\n",
        "            F.countDistinct(col(\"se.session_id\")).alias(\"user_cat_sessions\")\n",
        "        ) \\\n",
        "        .select(\n",
        "            col(\"user_id\"),\n",
        "            col(\"category_id\"),\n",
        "            col(\"user_cat_interactions\"),\n",
        "            col(\"user_cat_sessions\")\n",
        "        )\n",
        "    \n",
        "    # 4. User statistics\n",
        "    user_stats = train_events.groupBy(\"user_id\", \"session_id\").agg(\n",
        "        (F.max(\"ts\").cast(\"long\") - F.min(\"ts\").cast(\"long\")).alias(\"session_duration\")\n",
        "    ).groupBy(\"user_id\").agg(\n",
        "        F.countDistinct(\"session_id\").alias(\"total_sessions\"),\n",
        "        F.avg(\"session_duration\").alias(\"avg_session_duration\")\n",
        "    )\n",
        "    \n",
        "    # Join all features\n",
        "    features = base.alias(\"base\") \\\n",
        "        .join(\n",
        "            prefix_stats.alias(\"ps\"),\n",
        "            (col(\"base.session_id\") == col(\"ps.session_id\")) &\n",
        "            (col(\"base.atc_ts\") == col(\"ps.atc_ts\")) &\n",
        "            (col(\"base.cand_category_id\") == col(\"ps.cand_category_id\")),\n",
        "            \"left\"\n",
        "        ) \\\n",
        "        .join(\n",
        "            cat_pop.alias(\"cp\"),\n",
        "            col(\"base.cand_category_id\") == col(\"cp.category_id\"),\n",
        "            \"left\"\n",
        "        ) \\\n",
        "        .join(\n",
        "            user_cat_aff.alias(\"uca\"),\n",
        "            (col(\"base.user_id\") == col(\"uca.user_id\")) &\n",
        "            (col(\"base.cand_category_id\") == col(\"uca.category_id\")),\n",
        "            \"left\"\n",
        "        ) \\\n",
        "        .join(\n",
        "            user_stats.alias(\"us\"),\n",
        "            col(\"base.user_id\") == col(\"us.user_id\"),\n",
        "            \"left\"\n",
        "        )\n",
        "    \n",
        "    # Calculate derived features\n",
        "    features = features.select(\n",
        "        col(\"base.session_id\"),\n",
        "        col(\"base.atc_ts\"),\n",
        "        col(\"base.cand_category_id\").alias(\"category_id\"),\n",
        "        \n",
        "        # Prefix features\n",
        "        F.coalesce(col(\"ps.n_prefix_items\"), F.lit(0)).alias(\"n_prefix_items\"),\n",
        "        F.coalesce(col(\"ps.n_prefix_events\"), F.lit(0)).alias(\"n_prefix_events\"),\n",
        "        F.coalesce(col(\"ps.cat_count_in_prefix\"), F.lit(0)).alias(\"cat_count_in_prefix\"),\n",
        "        (F.coalesce(col(\"ps.cat_count_in_prefix\"), F.lit(0)) / \n",
        "         F.greatest(F.coalesce(col(\"ps.n_prefix_events\"), F.lit(1)), F.lit(1))).alias(\"cat_share_in_prefix\"),\n",
        "        F.coalesce(col(\"ps.recency_sec\"), F.lit(999999)).alias(\"recency_sec\"),\n",
        "        F.log1p(F.coalesce(col(\"ps.recency_sec\"), F.lit(999999))).alias(\"log_recency\"),\n",
        "        \n",
        "        # Time features\n",
        "        F.hour(col(\"base.atc_ts\")).alias(\"hour_of_day\"),\n",
        "        F.dayofweek(col(\"base.atc_ts\")).alias(\"day_of_week\"),\n",
        "        when(F.dayofweek(col(\"base.atc_ts\")).isin([1, 7]), 1).otherwise(0).alias(\"is_weekend\"),\n",
        "        F.coalesce(unix_timestamp(col(\"base.atc_ts\")) - unix_timestamp(col(\"ps.session_start\")), F.lit(0)).alias(\"time_since_session_start\"),\n",
        "        F.coalesce(col(\"ps.n_unique_cats_in_session\"), F.lit(0)).alias(\"session_cat_diversity\"),\n",
        "        \n",
        "        # Category popularity\n",
        "        F.coalesce(col(\"cp.global_pop\"), F.lit(1)).alias(\"cat_popularity\"),\n",
        "        F.log1p(F.coalesce(col(\"cp.global_pop\"), F.lit(1))).alias(\"log_cat_pop\"),\n",
        "        \n",
        "        # User-Category affinity\n",
        "        F.coalesce(col(\"uca.user_cat_interactions\"), F.lit(0)).alias(\"user_cat_hist\"),\n",
        "        F.log1p(F.coalesce(col(\"uca.user_cat_interactions\"), F.lit(0))).alias(\"log_user_cat_hist\"),\n",
        "        F.coalesce(col(\"uca.user_cat_sessions\"), F.lit(0)).alias(\"user_cat_sessions\"),\n",
        "        \n",
        "        # User statistics\n",
        "        F.coalesce(col(\"us.total_sessions\"), F.lit(0)).alias(\"user_total_sessions\"),\n",
        "        F.coalesce(col(\"us.avg_session_duration\"), F.lit(0)).alias(\"user_avg_session_dur\"),\n",
        "        \n",
        "        # Label\n",
        "        when(col(\"base.true_category_id\") == col(\"base.cand_category_id\"), 1).otherwise(0).alias(\"y\")\n",
        "    )\n",
        "    \n",
        "    # Count rows first (before adding embeddings)\n",
        "    n_rows = features.count()\n",
        "    \n",
        "    print(f\"    {split_name}: {n_rows:,} rows of base features\")\n",
        "    print(f\"    Adding 16-dimensional category embeddings...\")\n",
        "    \n",
        "    # Broadcast embedding dictionary to improve performance\n",
        "    emb_broadcast = spark.sparkContext.broadcast(cat_embeddings)\n",
        "    \n",
        "    # Define UDF to get a specific dimension of the embedding\n",
        "    def get_embedding_dim(cat_id, dim_idx):\n",
        "        emb_dict = emb_broadcast.value\n",
        "        if cat_id in emb_dict:\n",
        "            return float(emb_dict[cat_id][dim_idx])\n",
        "        else:\n",
        "            return 0.0\n",
        "    \n",
        "    # Register UDF\n",
        "    from pyspark.sql.types import FloatType\n",
        "    get_emb_udf = F.udf(get_embedding_dim, FloatType())\n",
        "    \n",
        "    # Add embedding dimensions one by one\n",
        "    for dim in range(16):\n",
        "        features = features.withColumn(\n",
        "            f'cat_emb_{dim}',\n",
        "            get_emb_udf(col(\"category_id\"), F.lit(dim))\n",
        "        )\n",
        "    \n",
        "    print(f\"    {split_name}: {n_rows:,} rows x {len(features.columns)} columns (with embeddings)\")\n",
        "    \n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Building train features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    train: 970,995 rows of base features\n",
            "    Adding 16-dimensional category embeddings...\n",
            "    train: 970,995 rows x 38 columns (with embeddings)\n",
            "  Building valid features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    valid: 551,781 rows of base features\n",
            "    Adding 16-dimensional category embeddings...\n",
            "    valid: 551,781 rows x 38 columns (with embeddings)\n",
            "✓ Feature engineering completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Build training and validation features\n",
        "X_train_spark = build_features_spark(atc_train, train_candidates, \"train\", TRAIN_END)\n",
        "X_valid_spark = build_features_spark(atc_valid, valid_candidates, \"valid\", TRAIN_END)\n",
        "\n",
        "print(\"✓ Feature engineering completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 7: Saving training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/04 20:08:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:09:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "25/12/04 20:11:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Training set saved to: data/processed/X_train_spark.parquet\n",
            "  Validation set saved to: data/processed/X_valid_spark.parquet\n",
            "✓ Data saving completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 7: Saving training data...\")\n",
        "\n",
        "train_output_path = f\"file://{(OUTPUT_DIR / 'X_train_spark.parquet').absolute()}\"\n",
        "valid_output_path = f\"file://{(OUTPUT_DIR / 'X_valid_spark.parquet').absolute()}\"\n",
        "\n",
        "X_train_spark.write.mode(\"overwrite\").parquet(train_output_path)\n",
        "X_valid_spark.write.mode(\"overwrite\").parquet(valid_output_path)\n",
        "\n",
        "print(f\"  Training set saved to: {OUTPUT_DIR / 'X_train_spark.parquet'}\")\n",
        "print(f\"  Validation set saved to: {OUTPUT_DIR / 'X_valid_spark.parquet'}\")\n",
        "print(\"✓ Data saving completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 8: Statistical Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Data Processing Completion Summary\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 809:==============================================>      (175 + 8) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Set:\n",
            "  Total rows: 970,995\n",
            "  Positive samples: 26,548 (2.73%)\n",
            "  Negative samples: 944,447 (97.27%)\n",
            "\n",
            "Validation Set:\n",
            "  Total rows: 551,781\n",
            "  Positive samples: 15,325 (2.78%)\n",
            "  Negative samples: 536,456 (97.22%)\n",
            "\n",
            "Feature Columns:\n",
            "  Total features: 34\n",
            "    - Base features: 18\n",
            "    - Category Embeddings: 16\n",
            "  Feature list: n_prefix_items, n_prefix_events, cat_count_in_prefix, cat_share_in_prefix, recency_sec, log_recency, hour_of_day, day_of_week, is_weekend, time_since_session_start, session_cat_diversity, cat_popularity, log_cat_pop, user_cat_hist, log_user_cat_hist, user_cat_sessions, user_total_sessions, user_avg_session_dur, cat_emb_0, cat_emb_1, cat_emb_2, cat_emb_3, cat_emb_4, cat_emb_5, cat_emb_6, cat_emb_7, cat_emb_8, cat_emb_9, cat_emb_10, cat_emb_11, cat_emb_12, cat_emb_13, cat_emb_14, cat_emb_15\n",
            "\n",
            "Next Steps:\n",
            "  1. Run ecommerce_classifier_v2_2.py (starting from LINE 398)\n",
            "  2. Load X_train_spark.parquet and X_valid_spark.parquet\n",
            "  3. Add interaction features (cat_pop_x_user_hist, recency_x_cat_count, etc. - 5 total)\n",
            "  4. Train LightGBM/XGBoost/CatBoost models\n",
            "\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Data Processing Completion Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate label distribution\n",
        "train_label_dist = X_train_spark.groupBy(\"y\").count().collect()\n",
        "valid_label_dist = X_valid_spark.groupBy(\"y\").count().collect()\n",
        "\n",
        "train_pos = [r['count'] for r in train_label_dist if r['y'] == 1][0] if any(r['y'] == 1 for r in train_label_dist) else 0\n",
        "train_total = sum(r['count'] for r in train_label_dist)\n",
        "valid_pos = [r['count'] for r in valid_label_dist if r['y'] == 1][0] if any(r['y'] == 1 for r in valid_label_dist) else 0\n",
        "valid_total = sum(r['count'] for r in valid_label_dist)\n",
        "\n",
        "print(f\"Training Set:\")\n",
        "print(f\"  Total rows: {train_total:,}\")\n",
        "print(f\"  Positive samples: {train_pos:,} ({train_pos/train_total*100:.2f}%)\")\n",
        "print(f\"  Negative samples: {train_total - train_pos:,} ({(train_total-train_pos)/train_total*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nValidation Set:\")\n",
        "print(f\"  Total rows: {valid_total:,}\")\n",
        "print(f\"  Positive samples: {valid_pos:,} ({valid_pos/valid_total*100:.2f}%)\")\n",
        "print(f\"  Negative samples: {valid_total - valid_pos:,} ({(valid_total-valid_pos)/valid_total*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nFeature Columns:\")\n",
        "feature_cols = [c for c in X_train_spark.columns if c not in ['session_id', 'atc_ts', 'category_id', 'y']]\n",
        "print(f\"  Total features: {len(feature_cols)}\")\n",
        "print(f\"    - Base features: 18\")\n",
        "print(f\"    - Category Embeddings: 16\")\n",
        "print(f\"  Feature list: {', '.join(feature_cols)}\")\n",
        "\n",
        "print(\"\\nNext Steps:\")\n",
        "print(\"  1. Run ecommerce_classifier_v2_2.py (starting from LINE 398)\")\n",
        "print(\"  2. Load X_train_spark.parquet and X_valid_spark.parquet\")\n",
        "print(\"  3. Add interaction features (cat_pop_x_user_hist, recency_x_cat_count, etc. - 5 total)\")\n",
        "print(\"  4. Train LightGBM/XGBoost/CatBoost models\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stop Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped\n"
          ]
        }
      ],
      "source": [
        "# Stop Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bigdata",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
