{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Data Processing Pipeline\n",
        "\n",
        "**Purpose**: Prepare training data\n",
        "\n",
        "**Author**: Bai, Gengyuan\n",
        "\n",
        "**Tasks**:\n",
        "1. Process raw data (events.csv, item_properties) using PySpark\n",
        "2. Build candidate sets (prefix, item covisitation, category covisitation, popularity, user history)\n",
        "3. Generate all features required for training\n",
        "4. Save as parquet files for subsequent training\n",
        "\n",
        "**Data Windows**:\n",
        "- Training: 2015-05-01 to 2015-07-01 (2 months)\n",
        "- Validation: 2015-07-01 to 2015-08-01 (1 month)\n",
        "- Session gap: 30 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, lag, unix_timestamp, when, concat, lit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Suppress all Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "\n",
        "# Set environment variables BEFORE creating Spark session\n",
        "os.environ['SPARK_LOCAL_DIRS'] = '/tmp/spark-temp'\n",
        "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
        "\n",
        "# Suppress Spark and Hadoop warnings\n",
        "os.environ['PYARROW_IGNORE_TIMEZONE'] = '1'\n",
        "\n",
        "# Configure Python logging to suppress all warnings\n",
        "logging.getLogger('py4j').setLevel(logging.CRITICAL)\n",
        "logging.getLogger('pyspark').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Redirect Java stderr to suppress log4j errors\n",
        "import subprocess\n",
        "import tempfile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PySpark Data Processing Pipeline Start\n",
            "================================================================================\n",
            "Start Time: 2025-12-08 13:52:45\n",
            "Training window: [2015-05-01, 2015-07-01)\n",
            "Validation window: [2015-07-01, 2015-08-01)\n",
            "Session gap: 30 minutes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "DATA_DIR = Path(\"data/raw\")\n",
        "OUTPUT_DIR = Path(\"data/processed\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_START = \"2015-05-01\"\n",
        "TRAIN_END = \"2015-07-01\"\n",
        "VALID_START = \"2015-07-01\"\n",
        "VALID_END = \"2015-08-01\"\n",
        "SESSION_GAP_MINUTES = 30\n",
        "\n",
        "# Start timing\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "pipeline_start_time = time.time()\n",
        "pipeline_start_datetime = datetime.now()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PySpark Data Processing Pipeline Start\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Start Time: {pipeline_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Training window: [{TRAIN_START}, {TRAIN_END})\")\n",
        "print(f\"Validation window: [{VALID_START}, {VALID_END})\")\n",
        "print(f\"Session gap: {SESSION_GAP_MINUTES} minutes\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 1: Initialize Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 1: Initializing Spark session...\n",
            "✓ Spark session created successfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from io import StringIO\n",
        "\n",
        "# Create a minimal log4j configuration to suppress all logging\n",
        "log4j_config_content = \"\"\"\n",
        "log4j.rootCategory=OFF\n",
        "log4j.logger.org.apache.spark=OFF\n",
        "log4j.logger.org.spark_project=OFF\n",
        "log4j.logger.org.apache.hadoop=OFF\n",
        "log4j.logger.akka=OFF\n",
        "log4j.logger.org.eclipse.jetty=OFF\n",
        "\"\"\"\n",
        "\n",
        "log4j_config_path = \"/tmp/spark-log4j.properties\"\n",
        "with open(log4j_config_path, \"w\") as f:\n",
        "    f.write(log4j_config_content)\n",
        "\n",
        "# Also create log4j2 configuration\n",
        "log4j2_config_content = \"\"\"\n",
        "status = OFF\n",
        "name = SparkConfig\n",
        "appender.console.type = Console\n",
        "appender.console.name = STDOUT\n",
        "appender.console.layout.type = PatternLayout\n",
        "rootLogger.level = OFF\n",
        "\"\"\"\n",
        "\n",
        "log4j2_config_path = \"/tmp/spark-log4j2.properties\"\n",
        "with open(log4j2_config_path, \"w\") as f:\n",
        "    f.write(log4j2_config_content)\n",
        "\n",
        "# Capture stderr to suppress Spark initialization warnings\n",
        "old_stderr = sys.stderr\n",
        "sys.stderr = StringIO()\n",
        "\n",
        "print(\"STEP 1: Initializing Spark session...\")\n",
        "\n",
        "# Create Spark session with comprehensive logging suppression\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Ecommerce_Training_Data_Preparation\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.local.dir\", \"/tmp/spark-temp\") \\\n",
        "    .config(\"spark.sql.debug.maxToStringFields\", \"1000\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
        "    .config(\"spark.driver.extraJavaOptions\", \n",
        "            f\"-Dlog4j.configuration=file:{log4j_config_path} \"\n",
        "            f\"-Dlog4j2.configurationFile=file:{log4j2_config_path} \"\n",
        "            \"-Dlog4j.logLevel=OFF \"\n",
        "            \"-Dlog4j2.level=OFF\") \\\n",
        "    .config(\"spark.executor.extraJavaOptions\", \n",
        "            f\"-Dlog4j.configuration=file:{log4j_config_path} \"\n",
        "            \"-Dlog4j.logLevel=OFF\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Immediately set log level to OFF (most restrictive)\n",
        "spark.sparkContext.setLogLevel(\"OFF\")\n",
        "\n",
        "# Suppress all Java/Scala logging through log4j\n",
        "try:\n",
        "    log4j = spark._jvm.org.apache.log4j\n",
        "    log4j.LogManager.getRootLogger().setLevel(log4j.Level.OFF)\n",
        "    log4j.Logger.getLogger(\"org\").setLevel(log4j.Level.OFF)\n",
        "    log4j.Logger.getLogger(\"akka\").setLevel(log4j.Level.OFF)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Restore stderr\n",
        "sys.stderr = old_stderr\n",
        "\n",
        "print(\"✓ Spark session created successfully\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 2: Load and Sessionize Events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 2: Loading events.csv and performing sessionization...\n",
            "  Events loaded: 1,902,445\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 2: Loading events.csv and performing sessionization...\")\n",
        "\n",
        "events_path = f\"file://{(DATA_DIR / 'events.csv').absolute()}\"\n",
        "events_df = spark.read.csv(events_path, header=True, inferSchema=True)\n",
        "\n",
        "# Convert timestamp (from milliseconds to seconds)\n",
        "events_df = events_df.withColumn(\n",
        "    \"ts\", \n",
        "    F.from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Filter time range (includes both training and validation windows)\n",
        "events_df = events_df.filter(\n",
        "    (col(\"ts\") >= F.lit(TRAIN_START).cast(\"timestamp\")) &\n",
        "    (col(\"ts\") < F.lit(VALID_END).cast(\"timestamp\"))\n",
        ")\n",
        "\n",
        "# Rename columns\n",
        "events_df = events_df.select(\n",
        "    col(\"visitorid\").cast(\"bigint\").alias(\"user_id\"),\n",
        "    col(\"ts\"),\n",
        "    col(\"itemid\").cast(\"bigint\").alias(\"item_id\"),\n",
        "    col(\"event\")\n",
        ")\n",
        "\n",
        "print(f\"  Events loaded: {events_df.count():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sessions generated: 1,194,255\n",
            "✓ Sessionization completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sessionization: 30-minute gap rule\n",
        "window_user_time = Window.partitionBy(\"user_id\").orderBy(\"ts\")\n",
        "\n",
        "events_df = events_df.withColumn(\n",
        "    \"prev_ts\",\n",
        "    lag(\"ts\").over(window_user_time)\n",
        ")\n",
        "\n",
        "events_df = events_df.withColumn(\n",
        "    \"time_gap_sec\",\n",
        "    when(\n",
        "        col(\"prev_ts\").isNotNull(),\n",
        "        unix_timestamp(\"ts\") - unix_timestamp(\"prev_ts\")\n",
        "    ).otherwise(0)\n",
        ")\n",
        "\n",
        "events_df = events_df.withColumn(\n",
        "    \"is_new_session\",\n",
        "    when(\n",
        "        (col(\"time_gap_sec\") > SESSION_GAP_MINUTES * 60) | col(\"prev_ts\").isNull(),\n",
        "        1\n",
        "    ).otherwise(0)\n",
        ")\n",
        "\n",
        "window_session = Window.partitionBy(\"user_id\").orderBy(\"ts\")\n",
        "events_df = events_df.withColumn(\n",
        "    \"session_num\",\n",
        "    F.sum(\"is_new_session\").over(window_session)\n",
        ")\n",
        "\n",
        "events_df = events_df.withColumn(\n",
        "    \"session_id\",\n",
        "    concat(col(\"user_id\").cast(\"string\"), lit(\"_\"), col(\"session_num\").cast(\"string\"))\n",
        ")\n",
        "\n",
        "# Clean intermediate columns\n",
        "events_df = events_df.select(\"session_id\", \"user_id\", \"ts\", \"item_id\", \"event\")\n",
        "\n",
        "# Cache to speed up subsequent operations\n",
        "events_df.cache()\n",
        "\n",
        "session_count = events_df.select(\"session_id\").distinct().count()\n",
        "print(f\"  Sessions generated: {session_count:,}\")\n",
        "print(\"✓ Sessionization completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 3: Load Item Category Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 3: Loading item properties and extracting category...\n",
            "  Items with category: 417,053\n",
            "✓ Category information loaded\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 3: Loading item properties and extracting category...\")\n",
        "\n",
        "part1_path = f\"file://{(DATA_DIR / 'item_properties_part1.csv').absolute()}\"\n",
        "part2_path = f\"file://{(DATA_DIR / 'item_properties_part2.csv').absolute()}\"\n",
        "\n",
        "props1 = spark.read.csv(part1_path, header=True, inferSchema=True)\n",
        "props2 = spark.read.csv(part2_path, header=True, inferSchema=True)\n",
        "\n",
        "# Merge two parts\n",
        "item_props = props1.union(props2)\n",
        "\n",
        "# Convert timestamp\n",
        "item_props = item_props.withColumn(\n",
        "    \"ts\",\n",
        "    F.from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Keep only categoryid property, and take the latest value\n",
        "item_props = item_props.filter(col(\"property\") == \"categoryid\")\n",
        "\n",
        "item_props = item_props.select(\n",
        "    col(\"itemid\").cast(\"bigint\").alias(\"item_id\"),\n",
        "    col(\"value\").cast(\"bigint\").alias(\"category_id\"),\n",
        "    col(\"ts\")\n",
        ")\n",
        "\n",
        "# For each item, take the latest category_id\n",
        "window_item = Window.partitionBy(\"item_id\").orderBy(F.desc(\"ts\"))\n",
        "item_props = item_props.withColumn(\"rn\", F.row_number().over(window_item))\n",
        "item_category = item_props.filter(col(\"rn\") == 1).select(\"item_id\", \"category_id\")\n",
        "\n",
        "item_category.cache()\n",
        "print(f\"  Items with category: {item_category.count():,}\")\n",
        "print(\"✓ Category information loaded\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 4: Extract Add-to-Cart Events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 4: Extracting add-to-cart events...\n",
            "  Training ATC events: 29,244\n",
            "  Validation ATC events: 17,151\n",
            "✓ ATC event extraction completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 4: Extracting add-to-cart events...\")\n",
        "\n",
        "# Join events with category\n",
        "atc_events = events_df.filter(col(\"event\") == \"addtocart\") \\\n",
        "    .join(item_category, \"item_id\", \"inner\") \\\n",
        "    .select(\n",
        "        col(\"session_id\"),\n",
        "        col(\"user_id\"),\n",
        "        col(\"ts\").alias(\"atc_ts\"),\n",
        "        col(\"item_id\"),\n",
        "        col(\"category_id\")\n",
        "    )\n",
        "\n",
        "atc_events.cache()\n",
        "\n",
        "# Split into training and validation sets\n",
        "atc_train = atc_events.filter(\n",
        "    (col(\"atc_ts\") >= F.lit(TRAIN_START).cast(\"timestamp\")) &\n",
        "    (col(\"atc_ts\") < F.lit(TRAIN_END).cast(\"timestamp\"))\n",
        ")\n",
        "\n",
        "atc_valid = atc_events.filter(\n",
        "    (col(\"atc_ts\") >= F.lit(VALID_START).cast(\"timestamp\")) &\n",
        "    (col(\"atc_ts\") < F.lit(VALID_END).cast(\"timestamp\"))\n",
        ")\n",
        "\n",
        "atc_train.cache()\n",
        "atc_valid.cache()\n",
        "\n",
        "n_atc_train = atc_train.count()\n",
        "n_atc_valid = atc_valid.count()\n",
        "\n",
        "print(f\"  Training ATC events: {n_atc_train:,}\")\n",
        "print(f\"  Validation ATC events: {n_atc_valid:,}\")\n",
        "print(\"✓ ATC event extraction completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 5: Build Candidate Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 5: Building candidate sets...\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 5: Building candidate sets...\")\n",
        "\n",
        "def build_candidates_spark(atc_df, split_name, train_cutoff_str):\n",
        "    \"\"\"\n",
        "    Build candidate category sets for given ATC events\n",
        "    Includes: prefix, item covisitation, category covisitation, popularity, user history\n",
        "    \"\"\"\n",
        "    print(f\"  Building {split_name} candidates...\")\n",
        "    \n",
        "    train_cutoff = F.lit(train_cutoff_str).cast(\"timestamp\")\n",
        "    \n",
        "    # 1. Prefix candidates: all categories in session prefix\n",
        "    prefix_cands = atc_df.alias(\"a\") \\\n",
        "        .join(\n",
        "            events_df.alias(\"se\"),\n",
        "            (col(\"a.session_id\") == col(\"se.session_id\")) & (col(\"se.ts\") < col(\"a.atc_ts\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .select(\n",
        "            col(\"a.session_id\"),\n",
        "            col(\"a.atc_ts\"),\n",
        "            col(\"ic.category_id\").alias(\"category_id\")\n",
        "        ).distinct()\n",
        "    \n",
        "    # 2. Item Co-visitation candidates\n",
        "    # Calculate item-item co-occurrence (before training cutoff)\n",
        "    train_events = events_df.filter(col(\"ts\") < train_cutoff)\n",
        "    \n",
        "    item_covis = train_events.alias(\"a\") \\\n",
        "        .join(\n",
        "            train_events.alias(\"b\"),\n",
        "            (col(\"a.session_id\") == col(\"b.session_id\")) & (col(\"a.item_id\") < col(\"b.item_id\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .groupBy(col(\"a.item_id\").alias(\"item_a\"), col(\"b.item_id\").alias(\"item_b\")) \\\n",
        "        .agg(F.count(\"*\").alias(\"covis\")) \\\n",
        "        .filter(col(\"covis\") >= 3)\n",
        "    \n",
        "    # For each ATC's prefix items, find co-occurring items and convert to categories\n",
        "    prefix_items = atc_df.alias(\"a\") \\\n",
        "        .join(\n",
        "            events_df.alias(\"se\"),\n",
        "            (col(\"a.session_id\") == col(\"se.session_id\")) & (col(\"se.ts\") < col(\"a.atc_ts\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .select(\n",
        "            col(\"a.session_id\"),\n",
        "            col(\"a.atc_ts\"),\n",
        "            col(\"se.item_id\")\n",
        "        )\n",
        "    \n",
        "    itemcovis_cands = prefix_items.alias(\"pi\") \\\n",
        "        .join(item_covis.alias(\"iv\"), col(\"pi.item_id\") == col(\"iv.item_a\"), \"inner\") \\\n",
        "        .join(item_category.alias(\"ic2\"), col(\"iv.item_b\") == col(\"ic2.item_id\"), \"inner\") \\\n",
        "        .groupBy(col(\"pi.session_id\"), col(\"pi.atc_ts\"), col(\"ic2.category_id\")) \\\n",
        "        .agg(F.max(\"iv.covis\").alias(\"max_covis\")) \\\n",
        "        .withColumn(\n",
        "            \"rn\",\n",
        "            F.row_number().over(\n",
        "                Window.partitionBy(\"session_id\", \"atc_ts\").orderBy(F.desc(\"max_covis\"))\n",
        "            )\n",
        "        ) \\\n",
        "        .filter(col(\"rn\") <= 15) \\\n",
        "        .select(col(\"session_id\"), col(\"atc_ts\"), col(\"ic2.category_id\").alias(\"category_id\"))\n",
        "    \n",
        "    # 3. Category Co-visitation candidates\n",
        "    # Calculate category-category co-occurrence\n",
        "    train_events_with_cat = train_events.alias(\"se\") \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .select(col(\"se.session_id\"), col(\"ic.category_id\"))\n",
        "    \n",
        "    cat_covis = train_events_with_cat.alias(\"a\") \\\n",
        "        .join(\n",
        "            train_events_with_cat.alias(\"b\"),\n",
        "            (col(\"a.session_id\") == col(\"b.session_id\")) & (col(\"a.category_id\") < col(\"b.category_id\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .groupBy(col(\"a.category_id\").alias(\"cat_a\"), col(\"b.category_id\").alias(\"cat_b\")) \\\n",
        "        .agg(F.countDistinct(\"a.session_id\").alias(\"cooccur\")) \\\n",
        "        .filter(col(\"cooccur\") >= 5)\n",
        "    \n",
        "    prefix_cats = atc_df.alias(\"a\") \\\n",
        "        .join(\n",
        "            events_df.alias(\"se\"),\n",
        "            (col(\"a.session_id\") == col(\"se.session_id\")) & (col(\"se.ts\") < col(\"a.atc_ts\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .select(\n",
        "            col(\"a.session_id\"),\n",
        "            col(\"a.atc_ts\"),\n",
        "            col(\"ic.category_id\")\n",
        "        )\n",
        "    \n",
        "    catcovis_cands = prefix_cats.alias(\"pc\") \\\n",
        "        .join(cat_covis.alias(\"cc\"), col(\"pc.category_id\") == col(\"cc.cat_a\"), \"inner\") \\\n",
        "        .groupBy(col(\"pc.session_id\"), col(\"pc.atc_ts\"), col(\"cc.cat_b\")) \\\n",
        "        .agg(F.max(\"cc.cooccur\").alias(\"max_cooccur\")) \\\n",
        "        .withColumn(\n",
        "            \"rn\",\n",
        "            F.row_number().over(\n",
        "                Window.partitionBy(\"session_id\", \"atc_ts\").orderBy(F.desc(\"max_cooccur\"))\n",
        "            )\n",
        "        ) \\\n",
        "        .filter(col(\"rn\") <= 10) \\\n",
        "        .select(col(\"session_id\"), col(\"atc_ts\"), col(\"cat_b\").alias(\"category_id\"))\n",
        "    \n",
        "    # 4. Popularity candidates: top 20 globally most popular categories\n",
        "    cat_pop = train_events.alias(\"se\") \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .groupBy(\"ic.category_id\") \\\n",
        "        .agg(F.count(\"*\").alias(\"cnt\")) \\\n",
        "        .orderBy(F.desc(\"cnt\")) \\\n",
        "        .limit(20)\n",
        "    \n",
        "    pop_cands = atc_df.alias(\"a\").crossJoin(cat_pop.select(col(\"category_id\").alias(\"pop_cat_id\"))) \\\n",
        "        .select(col(\"a.session_id\"), col(\"a.atc_ts\"), col(\"pop_cat_id\").alias(\"category_id\"))\n",
        "    \n",
        "    # 5. User History candidates: user's historically viewed categories (recent 10)\n",
        "    user_past_cats = train_events.alias(\"se\") \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .filter(col(\"se.ts\") < train_cutoff) \\\n",
        "        .groupBy(col(\"se.user_id\"), col(\"ic.category_id\")) \\\n",
        "        .agg(F.max(\"se.ts\").alias(\"last_seen\"))\n",
        "    \n",
        "    userhist_cands = atc_df.alias(\"a\") \\\n",
        "        .join(\n",
        "            user_past_cats.alias(\"upc\"),\n",
        "            (col(\"a.user_id\") == col(\"upc.user_id\")) & (col(\"upc.last_seen\") < col(\"a.atc_ts\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .withColumn(\n",
        "            \"rn\",\n",
        "            F.row_number().over(\n",
        "                Window.partitionBy(\"a.session_id\", \"a.atc_ts\").orderBy(F.desc(\"upc.last_seen\"))\n",
        "            )\n",
        "        ) \\\n",
        "        .filter(col(\"rn\") <= 10) \\\n",
        "        .select(col(\"a.session_id\"), col(\"a.atc_ts\"), col(\"upc.category_id\").alias(\"category_id\"))\n",
        "    \n",
        "    # Merge all candidates\n",
        "    all_candidates = prefix_cands \\\n",
        "        .union(itemcovis_cands) \\\n",
        "        .union(catcovis_cands) \\\n",
        "        .union(pop_cands) \\\n",
        "        .union(userhist_cands) \\\n",
        "        .distinct()\n",
        "    \n",
        "    n_cands = all_candidates.count()\n",
        "    print(f\"    {split_name}: {n_cands:,} candidates\")\n",
        "    \n",
        "    return all_candidates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Building train candidates...\n",
            "    train: 948,570 candidates\n",
            "  Building valid candidates...\n",
            "    valid: 538,331 candidates\n",
            "✓ Candidate set building completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Build training and validation candidate sets\n",
        "train_candidates = build_candidates_spark(atc_train, \"train\", TRAIN_END)\n",
        "valid_candidates = build_candidates_spark(atc_valid, \"valid\", TRAIN_END)\n",
        "\n",
        "train_candidates.cache()\n",
        "valid_candidates.cache()\n",
        "\n",
        "print(\"✓ Candidate set building completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 5.5: Training Category Embeddings (Word2Vec)...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Extracted 160,240 category sequences for training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Trained embeddings for 921 categories\n",
            "  Example: Category 1051 most similar categories: [(1192, 0.863), (218, 0.851), (626, 0.839), (1213, 0.821), (568, 0.804)]\n",
            "✓ Word2Vec training completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 5.5: Training Category Embeddings (Word2Vec)...\")\n",
        "\n",
        "# Extract category sequences for each session (for Word2Vec training)\n",
        "cat_seqs_spark = events_df.filter(col(\"ts\") < F.lit(TRAIN_END).cast(\"timestamp\")) \\\n",
        "    .join(item_category, \"item_id\", \"inner\") \\\n",
        "    .select(\"session_id\", \"ts\", \"category_id\") \\\n",
        "    .orderBy(\"session_id\", \"ts\")\n",
        "\n",
        "cat_seqs_spark = cat_seqs_spark.groupBy(\"session_id\").agg(\n",
        "    F.collect_list(\"category_id\").alias(\"cat_sequence\")\n",
        ")\n",
        "\n",
        "# Convert to Pandas to use gensim\n",
        "cat_seqs_pd = cat_seqs_spark.toPandas()\n",
        "\n",
        "# Prepare training data (convert to string lists)\n",
        "sequences = [[str(cat) for cat in seq if cat is not None] for seq in cat_seqs_pd['cat_sequence']]\n",
        "sequences = [seq for seq in sequences if len(seq) >= 2]  # Filter short sequences\n",
        "\n",
        "print(f\"  Extracted {len(sequences):,} category sequences for training\")\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=sequences,\n",
        "    vector_size=16,\n",
        "    window=5,\n",
        "    min_count=3,\n",
        "    workers=4,\n",
        "    sg=1,\n",
        "    epochs=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"  Trained embeddings for {len(w2v_model.wv)} categories\")\n",
        "\n",
        "# Create embedding lookup dictionary\n",
        "cat_embeddings = {int(cat): w2v_model.wv[cat] for cat in w2v_model.wv.index_to_key}\n",
        "\n",
        "# Show similarity check\n",
        "sample_cat = list(cat_embeddings.keys())[0]\n",
        "similar = w2v_model.wv.most_similar(str(sample_cat), topn=5)\n",
        "print(f\"  Example: Category {sample_cat} most similar categories: {[(int(c), round(s, 3)) for c, s in similar]}\")\n",
        "\n",
        "print(\"✓ Word2Vec training completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 6: Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 6: Feature engineering...\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 6: Feature engineering...\")\n",
        "\n",
        "def build_features_spark(atc_df, candidates_df, split_name, train_cutoff_str):\n",
        "    \"\"\"\n",
        "    Build all features required for training for the candidate set\n",
        "    \"\"\"\n",
        "    print(f\"  Building {split_name} features...\")\n",
        "    \n",
        "    train_cutoff = F.lit(train_cutoff_str).cast(\"timestamp\")\n",
        "    train_events = events_df.filter(col(\"ts\") < train_cutoff)\n",
        "    \n",
        "    # Join ATC with candidates\n",
        "    base = atc_df.alias(\"a\") \\\n",
        "        .join(\n",
        "            candidates_df.alias(\"c\"),\n",
        "            (col(\"a.session_id\") == col(\"c.session_id\")) & (col(\"a.atc_ts\") == col(\"c.atc_ts\")),\n",
        "            \"inner\"\n",
        "        ) \\\n",
        "        .select(\n",
        "            col(\"a.session_id\"),\n",
        "            col(\"a.user_id\"),\n",
        "            col(\"a.atc_ts\"),\n",
        "            col(\"a.category_id\").alias(\"true_category_id\"),\n",
        "            col(\"c.category_id\").alias(\"cand_category_id\")\n",
        "        )\n",
        "    \n",
        "    # 1. Prefix statistics features\n",
        "    prefix_events = base.alias(\"b\") \\\n",
        "        .join(\n",
        "            events_df.alias(\"se\"),\n",
        "            (col(\"b.session_id\") == col(\"se.session_id\")) & (col(\"se.ts\") < col(\"b.atc_ts\")),\n",
        "            \"left\"\n",
        "        ) \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"left\")\n",
        "    \n",
        "    prefix_stats = prefix_events.groupBy(\n",
        "        col(\"b.session_id\"), col(\"b.atc_ts\"), col(\"b.cand_category_id\")\n",
        "    ).agg(\n",
        "        F.countDistinct(col(\"se.item_id\")).alias(\"n_prefix_items\"),\n",
        "        F.count(col(\"se.item_id\")).alias(\"n_prefix_events\"),\n",
        "        F.sum(when(col(\"ic.category_id\") == col(\"b.cand_category_id\"), 1).otherwise(0)).alias(\"cat_count_in_prefix\"),\n",
        "        F.max(\n",
        "            when(col(\"ic.category_id\") == col(\"b.cand_category_id\"), \n",
        "                 unix_timestamp(col(\"b.atc_ts\")) - unix_timestamp(col(\"se.ts\")))\n",
        "        ).alias(\"recency_sec\"),\n",
        "        F.min(col(\"se.ts\")).alias(\"session_start\"),\n",
        "        F.countDistinct(col(\"ic.category_id\")).alias(\"n_unique_cats_in_session\")\n",
        "    ).select(\n",
        "        col(\"session_id\"),\n",
        "        col(\"atc_ts\"),\n",
        "        col(\"cand_category_id\"),\n",
        "        col(\"n_prefix_items\"),\n",
        "        col(\"n_prefix_events\"),\n",
        "        col(\"cat_count_in_prefix\"),\n",
        "        col(\"recency_sec\"),\n",
        "        col(\"session_start\"),\n",
        "        col(\"n_unique_cats_in_session\")\n",
        "    )\n",
        "    \n",
        "    # 2. Category global popularity\n",
        "    cat_pop = train_events.alias(\"se\") \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .groupBy(col(\"ic.category_id\")) \\\n",
        "        .agg(F.count(\"*\").alias(\"global_pop\")) \\\n",
        "        .select(\n",
        "            col(\"category_id\"),\n",
        "            col(\"global_pop\")\n",
        "        )\n",
        "    \n",
        "    # 3. User-Category affinity\n",
        "    user_cat_aff = train_events.alias(\"se\") \\\n",
        "        .join(item_category.alias(\"ic\"), col(\"se.item_id\") == col(\"ic.item_id\"), \"inner\") \\\n",
        "        .groupBy(col(\"se.user_id\"), col(\"ic.category_id\")) \\\n",
        "        .agg(\n",
        "            F.count(\"*\").alias(\"user_cat_interactions\"),\n",
        "            F.countDistinct(col(\"se.session_id\")).alias(\"user_cat_sessions\")\n",
        "        ) \\\n",
        "        .select(\n",
        "            col(\"user_id\"),\n",
        "            col(\"category_id\"),\n",
        "            col(\"user_cat_interactions\"),\n",
        "            col(\"user_cat_sessions\")\n",
        "        )\n",
        "    \n",
        "    # 4. User statistics\n",
        "    user_stats = train_events.groupBy(\"user_id\", \"session_id\").agg(\n",
        "        (F.max(\"ts\").cast(\"long\") - F.min(\"ts\").cast(\"long\")).alias(\"session_duration\")\n",
        "    ).groupBy(\"user_id\").agg(\n",
        "        F.countDistinct(\"session_id\").alias(\"total_sessions\"),\n",
        "        F.avg(\"session_duration\").alias(\"avg_session_duration\")\n",
        "    )\n",
        "    \n",
        "    # Join all features\n",
        "    features = base.alias(\"base\") \\\n",
        "        .join(\n",
        "            prefix_stats.alias(\"ps\"),\n",
        "            (col(\"base.session_id\") == col(\"ps.session_id\")) &\n",
        "            (col(\"base.atc_ts\") == col(\"ps.atc_ts\")) &\n",
        "            (col(\"base.cand_category_id\") == col(\"ps.cand_category_id\")),\n",
        "            \"left\"\n",
        "        ) \\\n",
        "        .join(\n",
        "            cat_pop.alias(\"cp\"),\n",
        "            col(\"base.cand_category_id\") == col(\"cp.category_id\"),\n",
        "            \"left\"\n",
        "        ) \\\n",
        "        .join(\n",
        "            user_cat_aff.alias(\"uca\"),\n",
        "            (col(\"base.user_id\") == col(\"uca.user_id\")) &\n",
        "            (col(\"base.cand_category_id\") == col(\"uca.category_id\")),\n",
        "            \"left\"\n",
        "        ) \\\n",
        "        .join(\n",
        "            user_stats.alias(\"us\"),\n",
        "            col(\"base.user_id\") == col(\"us.user_id\"),\n",
        "            \"left\"\n",
        "        )\n",
        "    \n",
        "    # Calculate derived features\n",
        "    features = features.select(\n",
        "        col(\"base.session_id\"),\n",
        "        col(\"base.atc_ts\"),\n",
        "        col(\"base.cand_category_id\").alias(\"category_id\"),\n",
        "        \n",
        "        # Prefix features\n",
        "        F.coalesce(col(\"ps.n_prefix_items\"), F.lit(0)).alias(\"n_prefix_items\"),\n",
        "        F.coalesce(col(\"ps.n_prefix_events\"), F.lit(0)).alias(\"n_prefix_events\"),\n",
        "        F.coalesce(col(\"ps.cat_count_in_prefix\"), F.lit(0)).alias(\"cat_count_in_prefix\"),\n",
        "        (F.coalesce(col(\"ps.cat_count_in_prefix\"), F.lit(0)) / \n",
        "         F.greatest(F.coalesce(col(\"ps.n_prefix_events\"), F.lit(1)), F.lit(1))).alias(\"cat_share_in_prefix\"),\n",
        "        F.coalesce(col(\"ps.recency_sec\"), F.lit(999999)).alias(\"recency_sec\"),\n",
        "        F.log1p(F.coalesce(col(\"ps.recency_sec\"), F.lit(999999))).alias(\"log_recency\"),\n",
        "        \n",
        "        # Time features\n",
        "        F.hour(col(\"base.atc_ts\")).alias(\"hour_of_day\"),\n",
        "        F.dayofweek(col(\"base.atc_ts\")).alias(\"day_of_week\"),\n",
        "        when(F.dayofweek(col(\"base.atc_ts\")).isin([1, 7]), 1).otherwise(0).alias(\"is_weekend\"),\n",
        "        F.coalesce(unix_timestamp(col(\"base.atc_ts\")) - unix_timestamp(col(\"ps.session_start\")), F.lit(0)).alias(\"time_since_session_start\"),\n",
        "        F.coalesce(col(\"ps.n_unique_cats_in_session\"), F.lit(0)).alias(\"session_cat_diversity\"),\n",
        "        \n",
        "        # Category popularity\n",
        "        F.coalesce(col(\"cp.global_pop\"), F.lit(1)).alias(\"cat_popularity\"),\n",
        "        F.log1p(F.coalesce(col(\"cp.global_pop\"), F.lit(1))).alias(\"log_cat_pop\"),\n",
        "        \n",
        "        # User-Category affinity\n",
        "        F.coalesce(col(\"uca.user_cat_interactions\"), F.lit(0)).alias(\"user_cat_hist\"),\n",
        "        F.log1p(F.coalesce(col(\"uca.user_cat_interactions\"), F.lit(0))).alias(\"log_user_cat_hist\"),\n",
        "        F.coalesce(col(\"uca.user_cat_sessions\"), F.lit(0)).alias(\"user_cat_sessions\"),\n",
        "        \n",
        "        # User statistics\n",
        "        F.coalesce(col(\"us.total_sessions\"), F.lit(0)).alias(\"user_total_sessions\"),\n",
        "        F.coalesce(col(\"us.avg_session_duration\"), F.lit(0)).alias(\"user_avg_session_dur\"),\n",
        "        \n",
        "        # Label\n",
        "        when(col(\"base.true_category_id\") == col(\"base.cand_category_id\"), 1).otherwise(0).alias(\"y\")\n",
        "    )\n",
        "    \n",
        "    # Count rows first (before adding embeddings)\n",
        "    n_rows = features.count()\n",
        "    \n",
        "    print(f\"    {split_name}: {n_rows:,} rows of base features\")\n",
        "    print(f\"    Adding 16-dimensional category embeddings...\")\n",
        "    \n",
        "    # Broadcast embedding dictionary to improve performance\n",
        "    emb_broadcast = spark.sparkContext.broadcast(cat_embeddings)\n",
        "    \n",
        "    # Define UDF to get a specific dimension of the embedding\n",
        "    def get_embedding_dim(cat_id, dim_idx):\n",
        "        emb_dict = emb_broadcast.value\n",
        "        if cat_id in emb_dict:\n",
        "            return float(emb_dict[cat_id][dim_idx])\n",
        "        else:\n",
        "            return 0.0\n",
        "    \n",
        "    # Register UDF\n",
        "    from pyspark.sql.types import FloatType\n",
        "    get_emb_udf = F.udf(get_embedding_dim, FloatType())\n",
        "    \n",
        "    # Add embedding dimensions one by one\n",
        "    for dim in range(16):\n",
        "        features = features.withColumn(\n",
        "            f'cat_emb_{dim}',\n",
        "            get_emb_udf(col(\"category_id\"), F.lit(dim))\n",
        "        )\n",
        "    \n",
        "    print(f\"    {split_name}: {n_rows:,} rows x {len(features.columns)} columns (with embeddings)\")\n",
        "    \n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Building train features...\n",
            "    train: 970,995 rows of base features\n",
            "    Adding 16-dimensional category embeddings...\n",
            "    train: 970,995 rows x 38 columns (with embeddings)\n",
            "  Building valid features...\n",
            "    valid: 551,775 rows of base features\n",
            "    Adding 16-dimensional category embeddings...\n",
            "    valid: 551,775 rows x 38 columns (with embeddings)\n",
            "✓ Feature engineering completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Build training and validation features\n",
        "X_train_spark = build_features_spark(atc_train, train_candidates, \"train\", TRAIN_END)\n",
        "X_valid_spark = build_features_spark(atc_valid, valid_candidates, \"valid\", TRAIN_END)\n",
        "\n",
        "print(\"✓ Feature engineering completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 7: Saving training data...\n",
            "  Training set saved to: data/processed/X_train_spark.parquet\n",
            "  Validation set saved to: data/processed/X_valid_spark.parquet\n",
            "✓ Data saving completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"STEP 7: Saving training data...\")\n",
        "\n",
        "# Optimize Spark configurations for saving large DataFrames\n",
        "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"1000\")\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\")\n",
        "\n",
        "train_output_path = f\"file://{(OUTPUT_DIR / 'X_train_spark.parquet').absolute()}\"\n",
        "valid_output_path = f\"file://{(OUTPUT_DIR / 'X_valid_spark.parquet').absolute()}\"\n",
        "\n",
        "# Save with optimized settings\n",
        "X_train_spark.write.mode(\"overwrite\").option(\"maxRecordsPerFile\", 50000).parquet(train_output_path)\n",
        "X_valid_spark.write.mode(\"overwrite\").option(\"maxRecordsPerFile\", 50000).parquet(valid_output_path)\n",
        "\n",
        "print(f\"  Training set saved to: {OUTPUT_DIR / 'X_train_spark.parquet'}\")\n",
        "print(f\"  Validation set saved to: {OUTPUT_DIR / 'X_valid_spark.parquet'}\")\n",
        "print(\"✓ Data saving completed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 8: Verify output Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Reading Generated Parquet Files\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "TRAINING DATA SCHEMA\n",
            "================================================================================\n",
            "root\n",
            " |-- session_id: string (nullable = true)\n",
            " |-- atc_ts: timestamp (nullable = true)\n",
            " |-- category_id: long (nullable = true)\n",
            " |-- n_prefix_items: long (nullable = true)\n",
            " |-- n_prefix_events: long (nullable = true)\n",
            " |-- cat_count_in_prefix: long (nullable = true)\n",
            " |-- cat_share_in_prefix: double (nullable = true)\n",
            " |-- recency_sec: long (nullable = true)\n",
            " |-- log_recency: double (nullable = true)\n",
            " |-- hour_of_day: integer (nullable = true)\n",
            " |-- day_of_week: integer (nullable = true)\n",
            " |-- is_weekend: integer (nullable = true)\n",
            " |-- time_since_session_start: long (nullable = true)\n",
            " |-- session_cat_diversity: long (nullable = true)\n",
            " |-- cat_popularity: long (nullable = true)\n",
            " |-- log_cat_pop: double (nullable = true)\n",
            " |-- user_cat_hist: long (nullable = true)\n",
            " |-- log_user_cat_hist: double (nullable = true)\n",
            " |-- user_cat_sessions: long (nullable = true)\n",
            " |-- user_total_sessions: long (nullable = true)\n",
            " |-- user_avg_session_dur: double (nullable = true)\n",
            " |-- y: integer (nullable = true)\n",
            " |-- cat_emb_0: float (nullable = true)\n",
            " |-- cat_emb_1: float (nullable = true)\n",
            " |-- cat_emb_2: float (nullable = true)\n",
            " |-- cat_emb_3: float (nullable = true)\n",
            " |-- cat_emb_4: float (nullable = true)\n",
            " |-- cat_emb_5: float (nullable = true)\n",
            " |-- cat_emb_6: float (nullable = true)\n",
            " |-- cat_emb_7: float (nullable = true)\n",
            " |-- cat_emb_8: float (nullable = true)\n",
            " |-- cat_emb_9: float (nullable = true)\n",
            " |-- cat_emb_10: float (nullable = true)\n",
            " |-- cat_emb_11: float (nullable = true)\n",
            " |-- cat_emb_12: float (nullable = true)\n",
            " |-- cat_emb_13: float (nullable = true)\n",
            " |-- cat_emb_14: float (nullable = true)\n",
            " |-- cat_emb_15: float (nullable = true)\n",
            "\n",
            "\n",
            "================================================================================\n",
            "VALIDATION DATA SCHEMA\n",
            "================================================================================\n",
            "root\n",
            " |-- session_id: string (nullable = true)\n",
            " |-- atc_ts: timestamp (nullable = true)\n",
            " |-- category_id: long (nullable = true)\n",
            " |-- n_prefix_items: long (nullable = true)\n",
            " |-- n_prefix_events: long (nullable = true)\n",
            " |-- cat_count_in_prefix: long (nullable = true)\n",
            " |-- cat_share_in_prefix: double (nullable = true)\n",
            " |-- recency_sec: long (nullable = true)\n",
            " |-- log_recency: double (nullable = true)\n",
            " |-- hour_of_day: integer (nullable = true)\n",
            " |-- day_of_week: integer (nullable = true)\n",
            " |-- is_weekend: integer (nullable = true)\n",
            " |-- time_since_session_start: long (nullable = true)\n",
            " |-- session_cat_diversity: long (nullable = true)\n",
            " |-- cat_popularity: long (nullable = true)\n",
            " |-- log_cat_pop: double (nullable = true)\n",
            " |-- user_cat_hist: long (nullable = true)\n",
            " |-- log_user_cat_hist: double (nullable = true)\n",
            " |-- user_cat_sessions: long (nullable = true)\n",
            " |-- user_total_sessions: long (nullable = true)\n",
            " |-- user_avg_session_dur: double (nullable = true)\n",
            " |-- y: integer (nullable = true)\n",
            " |-- cat_emb_0: float (nullable = true)\n",
            " |-- cat_emb_1: float (nullable = true)\n",
            " |-- cat_emb_2: float (nullable = true)\n",
            " |-- cat_emb_3: float (nullable = true)\n",
            " |-- cat_emb_4: float (nullable = true)\n",
            " |-- cat_emb_5: float (nullable = true)\n",
            " |-- cat_emb_6: float (nullable = true)\n",
            " |-- cat_emb_7: float (nullable = true)\n",
            " |-- cat_emb_8: float (nullable = true)\n",
            " |-- cat_emb_9: float (nullable = true)\n",
            " |-- cat_emb_10: float (nullable = true)\n",
            " |-- cat_emb_11: float (nullable = true)\n",
            " |-- cat_emb_12: float (nullable = true)\n",
            " |-- cat_emb_13: float (nullable = true)\n",
            " |-- cat_emb_14: float (nullable = true)\n",
            " |-- cat_emb_15: float (nullable = true)\n",
            "\n",
            "\n",
            "================================================================================\n",
            "DATASET STATISTICS\n",
            "================================================================================\n",
            "Training rows: 970,995\n",
            "Validation rows: 551,775\n",
            "Number of columns: 38\n",
            "Column names: session_id, atc_ts, category_id, n_prefix_items, n_prefix_events, cat_count_in_prefix, cat_share_in_prefix, recency_sec, log_recency, hour_of_day, day_of_week, is_weekend, time_since_session_start, session_cat_diversity, cat_popularity, log_cat_pop, user_cat_hist, log_user_cat_hist, user_cat_sessions, user_total_sessions, user_avg_session_dur, y, cat_emb_0, cat_emb_1, cat_emb_2, cat_emb_3, cat_emb_4, cat_emb_5, cat_emb_6, cat_emb_7, cat_emb_8, cat_emb_9, cat_emb_10, cat_emb_11, cat_emb_12, cat_emb_13, cat_emb_14, cat_emb_15\n",
            "\n",
            "================================================================================\n",
            "TRAINING DATA - First 10 Rows\n",
            "================================================================================\n",
            "+----------+-------------------+-----------+--------------+---------------+-------------------+-------------------+-----------+------------------+-----------+-----------+----------+------------------------+---------------------+--------------+------------------+-------------+-----------------+-----------------+-------------------+--------------------+---+-----------+-----------+-------------+------------+---------+-----------+-----------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+----------+\n",
            "|session_id|atc_ts             |category_id|n_prefix_items|n_prefix_events|cat_count_in_prefix|cat_share_in_prefix|recency_sec|log_recency       |hour_of_day|day_of_week|is_weekend|time_since_session_start|session_cat_diversity|cat_popularity|log_cat_pop       |user_cat_hist|log_user_cat_hist|user_cat_sessions|user_total_sessions|user_avg_session_dur|y  |cat_emb_0  |cat_emb_1  |cat_emb_2    |cat_emb_3   |cat_emb_4|cat_emb_5  |cat_emb_6  |cat_emb_7  |cat_emb_8  |cat_emb_9  |cat_emb_10  |cat_emb_11 |cat_emb_12  |cat_emb_13 |cat_emb_14 |cat_emb_15|\n",
            "+----------+-------------------+-----------+--------------+---------------+-------------------+-------------------+-----------+------------------+-----------+-----------+----------+------------------------+---------------------+--------------+------------------+-------------+-----------------+-----------------+-------------------+--------------------+---+-----------+-----------+-------------+------------+---------+-----------+-----------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+----------+\n",
            "|7969_1    |2015-05-27 19:44:23|1248       |1             |1              |0                  |0.0                |999999     |13.815510557964274|19         |4          |0         |807                     |1                    |6521          |8.782936356349264 |0            |0.0              |0                |5                  |776.0               |0  |-0.3563315 |0.48952854 |-0.0025500678|0.6538355   |1.7831018|-0.9994619 |-1.0100222 |-0.8544958 |-1.1903448 |-1.3459611 |-0.071864285|-0.6838312 |-1.4556063  |-0.5441784 |-0.88377273|1.2019216 |\n",
            "|7969_1    |2015-05-27 19:44:23|1483       |1             |1              |0                  |0.0                |999999     |13.815510557964274|19         |4          |0         |807                     |1                    |25278         |10.137729290516372|0            |0.0              |0                |5                  |776.0               |0  |0.045796838|0.6733938  |0.7671803    |0.8933412   |1.3478558|-0.9609856 |-0.7707558 |-0.0939024 |-0.49715856|0.20671658 |0.4552296   |-0.41764867|-0.225022   |0.7597002  |0.06384835 |0.2303787 |\n",
            "|7969_1    |2015-05-27 19:44:23|84         |1             |1              |0                  |0.0                |999999     |13.815510557964274|19         |4          |0         |807                     |1                    |8948          |9.099297073182859 |0            |0.0              |0                |5                  |776.0               |0  |0.07383493 |0.8471243  |0.4029023    |0.1981063   |2.2832177|-1.6030908 |-0.4767821 |0.09629028 |0.37608674 |-0.30671358|-0.5574206  |-0.375664  |-1.3059735  |-0.4520491 |-0.22393708|1.1820965 |\n",
            "|7969_1    |2015-05-27 19:44:23|646        |1             |1              |0                  |0.0                |999999     |13.815510557964274|19         |4          |0         |807                     |1                    |15068         |9.620394932418154 |0            |0.0              |0                |5                  |776.0               |0  |-0.3065954 |0.70825434 |0.12498125   |-0.19941632 |1.8282238|-1.0307274 |0.3190789  |-1.2357213 |-0.20198923|0.6125204  |0.57452184  |0.28613043 |0.38751233  |-0.40094805|-1.6870921 |0.80256283|\n",
            "|7969_1    |2015-05-27 19:44:23|683        |1             |1              |0                  |0.0                |999999     |13.815510557964274|19         |4          |0         |807                     |1                    |24976         |10.125710680390595|0            |0.0              |0                |5                  |776.0               |0  |0.13161112 |-0.1604588 |0.87996215   |-0.25099534 |0.9212608|-1.1134655 |0.2304762  |-0.49337855|0.32155713 |0.084010065|-0.012253182|0.46446356 |-1.0108451  |-0.6868879 |-1.937471  |0.29735297|\n",
            "|7969_1    |2015-05-27 19:44:23|1279       |1             |1              |0                  |0.0                |999999     |13.815510557964274|19         |4          |0         |807                     |1                    |20162         |9.911604520638075 |0            |0.0              |0                |5                  |776.0               |0  |-0.21821518|-0.50035805|0.62071234   |-0.26655823 |1.2460358|-0.8101587 |0.47172192 |-0.7200393 |0.384693   |-0.596792  |0.48886502  |0.05103209 |-0.042510927|-0.17006789|-2.0015204 |0.91360813|\n",
            "|7969_1    |2015-05-27 19:44:23|1375       |1             |1              |0                  |0.0                |999999     |13.815510557964274|19         |4          |0         |807                     |1                    |7794          |8.961237781491876 |0            |0.0              |0                |5                  |776.0               |0  |-0.89433116|1.9496572  |0.025856176  |1.157535    |0.9674013|-0.23356499|-1.391014  |-0.8795765 |-0.28753808|-0.25637907|-0.016098056|-0.67364454|-1.3122872  |0.38808143 |-1.2972896 |0.40102568|\n",
            "|7969_1    |2015-05-27 19:44:23|1625       |1             |1              |0                  |0.0                |999999     |13.815510557964274|19         |4          |0         |807                     |1                    |7638          |8.94102198354136  |0            |0.0              |0                |5                  |776.0               |0  |-1.6389129 |-0.9051447 |0.55772704   |1.1169075   |1.5171947|-1.7933862 |0.5920348  |0.030007472|-0.58335257|0.18812487 |-0.353932   |1.1690832  |-0.23139612 |-0.21607125|-2.2888436 |0.38086566|\n",
            "|7969_1    |2015-05-27 19:44:23|196        |1             |1              |0                  |0.0                |999999     |13.815510557964274|19         |4          |0         |807                     |1                    |10929         |9.299266581170585 |0            |0.0              |0                |5                  |776.0               |0  |-1.706838  |0.405439   |0.6070744    |-0.015538533|0.7807394|-0.9946188 |-0.78309125|-0.25017917|-0.41101384|-1.2129644 |-0.9187219  |-0.66000634|-0.16169485 |-0.5284156 |-0.34704313|0.4461594 |\n",
            "|7969_1    |2015-05-27 19:44:23|1135       |1             |1              |0                  |0.0                |999999     |13.815510557964274|19         |4          |0         |807                     |1                    |10358         |9.245610986058102 |0            |0.0              |0                |5                  |776.0               |0  |0.37320533 |1.2211379  |-0.3561217   |-0.081610285|2.1435616|-0.34281173|-0.44669497|-0.17576987|0.78658956 |-0.18667953|-0.30661434 |-0.5840734 |-0.9967679  |-0.12395305|-0.8359512 |1.186296  |\n",
            "+----------+-------------------+-----------+--------------+---------------+-------------------+-------------------+-----------+------------------+-----------+-----------+----------+------------------------+---------------------+--------------+------------------+-------------+-----------------+-----------------+-------------------+--------------------+---+-----------+-----------+-------------+------------+---------+-----------+-----------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "================================================================================\n",
            "VALIDATION DATA - First 10 Rows\n",
            "================================================================================\n",
            "+----------+-------------------+-----------+--------------+---------------+-------------------+-------------------+-----------+------------------+-----------+-----------+----------+------------------------+---------------------+--------------+------------------+-------------+-----------------+-----------------+-------------------+--------------------+---+-----------+----------+----------+------------+---------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+-----------+-----------+----------+\n",
            "|session_id|atc_ts             |category_id|n_prefix_items|n_prefix_events|cat_count_in_prefix|cat_share_in_prefix|recency_sec|log_recency       |hour_of_day|day_of_week|is_weekend|time_since_session_start|session_cat_diversity|cat_popularity|log_cat_pop       |user_cat_hist|log_user_cat_hist|user_cat_sessions|user_total_sessions|user_avg_session_dur|y  |cat_emb_0  |cat_emb_1 |cat_emb_2 |cat_emb_3   |cat_emb_4|cat_emb_5  |cat_emb_6  |cat_emb_7  |cat_emb_8   |cat_emb_9  |cat_emb_10 |cat_emb_11 |cat_emb_12 |cat_emb_13 |cat_emb_14 |cat_emb_15|\n",
            "+----------+-------------------+-----------+--------------+---------------+-------------------+-------------------+-----------+------------------+-----------+-----------+----------+------------------------+---------------------+--------------+------------------+-------------+-----------------+-----------------+-------------------+--------------------+---+-----------+----------+----------+------------+---------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+-----------+-----------+----------+\n",
            "|11193_2   |2015-07-13 17:26:55|196        |1             |1              |0                  |0.0                |999999     |13.815510557964274|17         |2          |0         |8                       |1                    |10929         |9.299266581170585 |0            |0.0              |0                |0                  |0.0                 |0  |-1.706838  |0.405439  |0.6070744 |-0.015538533|0.7807394|-0.9946188 |-0.78309125|-0.25017917|-0.41101384 |-1.2129644 |-0.9187219 |-0.66000634|-0.16169485|-0.5284156 |-0.34704313|0.4461594 |\n",
            "|11193_2   |2015-07-13 17:26:47|196        |0             |0              |0                  |0.0                |999999     |13.815510557964274|17         |2          |0         |0                       |0                    |10929         |9.299266581170585 |0            |0.0              |0                |0                  |0.0                 |0  |-1.706838  |0.405439  |0.6070744 |-0.015538533|0.7807394|-0.9946188 |-0.78309125|-0.25017917|-0.41101384 |-1.2129644 |-0.9187219 |-0.66000634|-0.16169485|-0.5284156 |-0.34704313|0.4461594 |\n",
            "|11193_2   |2015-07-13 17:26:47|1051       |0             |0              |0                  |0.0                |999999     |13.815510557964274|17         |2          |0         |0                       |0                    |29300         |10.285376924115587|0            |0.0              |0                |0                  |0.0                 |0  |-0.55171376|1.5059713 |0.20879163|0.21008378  |1.4800618|-0.3802817 |-1.1226728 |0.4340892  |-0.38299066 |-0.59037125|-0.2971248 |0.041822553|-0.84359795|-0.07202523|-0.41900924|0.18181336|\n",
            "|11193_2   |2015-07-13 17:26:55|1051       |1             |1              |0                  |0.0                |999999     |13.815510557964274|17         |2          |0         |8                       |1                    |29300         |10.285376924115587|0            |0.0              |0                |0                  |0.0                 |0  |-0.55171376|1.5059713 |0.20879163|0.21008378  |1.4800618|-0.3802817 |-1.1226728 |0.4340892  |-0.38299066 |-0.59037125|-0.2971248 |0.041822553|-0.84359795|-0.07202523|-0.41900924|0.18181336|\n",
            "|11193_2   |2015-07-13 17:26:55|1483       |1             |1              |0                  |0.0                |999999     |13.815510557964274|17         |2          |0         |8                       |1                    |25278         |10.137729290516372|0            |0.0              |0                |0                  |0.0                 |0  |0.045796838|0.6733938 |0.7671803 |0.8933412   |1.3478558|-0.9609856 |-0.7707558 |-0.0939024 |-0.49715856 |0.20671658 |0.4552296  |-0.41764867|-0.225022  |0.7597002  |0.06384835 |0.2303787 |\n",
            "|11193_2   |2015-07-13 17:26:47|1483       |0             |0              |0                  |0.0                |999999     |13.815510557964274|17         |2          |0         |0                       |0                    |25278         |10.137729290516372|0            |0.0              |0                |0                  |0.0                 |0  |0.045796838|0.6733938 |0.7671803 |0.8933412   |1.3478558|-0.9609856 |-0.7707558 |-0.0939024 |-0.49715856 |0.20671658 |0.4552296  |-0.41764867|-0.225022  |0.7597002  |0.06384835 |0.2303787 |\n",
            "|11193_2   |2015-07-13 17:26:55|1625       |1             |1              |0                  |0.0                |999999     |13.815510557964274|17         |2          |0         |8                       |1                    |7638          |8.94102198354136  |0            |0.0              |0                |0                  |0.0                 |0  |-1.6389129 |-0.9051447|0.55772704|1.1169075   |1.5171947|-1.7933862 |0.5920348  |0.030007472|-0.58335257 |0.18812487 |-0.353932  |1.1690832  |-0.23139612|-0.21607125|-2.2888436 |0.38086566|\n",
            "|11193_2   |2015-07-13 17:26:47|1625       |0             |0              |0                  |0.0                |999999     |13.815510557964274|17         |2          |0         |0                       |0                    |7638          |8.94102198354136  |0            |0.0              |0                |0                  |0.0                 |0  |-1.6389129 |-0.9051447|0.55772704|1.1169075   |1.5171947|-1.7933862 |0.5920348  |0.030007472|-0.58335257 |0.18812487 |-0.353932  |1.1690832  |-0.23139612|-0.21607125|-2.2888436 |0.38086566|\n",
            "|11193_2   |2015-07-13 17:26:55|1663       |1             |1              |0                  |0.0                |999999     |13.815510557964274|17         |2          |0         |8                       |1                    |2451          |7.804659297056102 |0            |0.0              |0                |0                  |0.0                 |0  |-1.2867532 |1.4183451 |2.2016416 |-0.4838078  |1.519758 |-1.0160135 |0.30217183 |0.96343213 |-0.010207341|0.7883119  |-0.11061478|0.39717078 |-0.34320423|-1.3662125 |-0.9834996 |0.3490779 |\n",
            "|11193_2   |2015-07-13 17:26:55|333        |1             |1              |0                  |0.0                |999999     |13.815510557964274|17         |2          |0         |8                       |1                    |9726          |9.18266080286944  |0            |0.0              |0                |0                  |0.0                 |0  |1.1460071  |1.0630711 |1.3013469 |-0.059077527|1.6138629|-0.09008229|-0.7141269 |0.77958214 |-1.1419003  |0.47263703 |-0.15813959|-0.75200987|0.73542434 |-0.0949771 |-1.1107446 |0.26002055|\n",
            "+----------+-------------------+-----------+--------------+---------------+-------------------+-------------------+-----------+------------------+-----------+-----------+----------+------------------------+---------------------+--------------+------------------+-------------+-----------------+-----------------+-------------------+--------------------+---+-----------+----------+----------+------------+---------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+-----------+-----------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "================================================================================\n",
            "LABEL DISTRIBUTION\n",
            "================================================================================\n",
            "\n",
            "Training set:\n",
            "+---+------+\n",
            "|  y| count|\n",
            "+---+------+\n",
            "|  0|944447|\n",
            "|  1| 26548|\n",
            "+---+------+\n",
            "\n",
            "Validation set:\n",
            "+---+------+\n",
            "|  y| count|\n",
            "+---+------+\n",
            "|  0|536450|\n",
            "|  1| 15325|\n",
            "+---+------+\n",
            "\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS - Key Features\n",
            "================================================================================\n",
            "+-------+------------------+------------------+-------------------+-----------------+------------------+------------------+\n",
            "|summary|    n_prefix_items|   n_prefix_events|cat_count_in_prefix|   cat_popularity|     user_cat_hist|       recency_sec|\n",
            "+-------+------------------+------------------+-------------------+-----------------+------------------+------------------+\n",
            "|  count|            970995|            970995|             970995|           970995|            970995|            970995|\n",
            "|   mean| 5.988889747115072|  9.89517762707326|0.22531526938861682|9933.275662593525|1.7070839705662748| 924582.5699380533|\n",
            "| stddev|12.811402174604046|22.479989020302163| 1.5317604295367397| 8259.57777523644|6.0357641114836635|263608.47561874427|\n",
            "|    min|                 0|                 0|                  0|                2|                 0|                 1|\n",
            "|    max|               206|               352|                115|            29507|               186|            999999|\n",
            "+-------+------------------+------------------+-------------------+-----------------+------------------+------------------+\n",
            "\n",
            "\n",
            "✓ Data verification completed successfully!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Reading Generated Parquet Files\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Read the saved parquet files\n",
        "train_output_path = f\"file://{(OUTPUT_DIR / 'X_train_spark.parquet').absolute()}\"\n",
        "df_train = spark.read.parquet(train_output_path)\n",
        "\n",
        "valid_output_path = f\"file://{(OUTPUT_DIR / 'X_valid_spark.parquet').absolute()}\"\n",
        "df_valid = spark.read.parquet(valid_output_path)\n",
        "\n",
        "# Display Schema\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TRAINING DATA SCHEMA\")\n",
        "print(\"=\" * 80)\n",
        "df_train.printSchema()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VALIDATION DATA SCHEMA\")\n",
        "print(\"=\" * 80)\n",
        "df_valid.printSchema()\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATASET STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Training rows: {df_train.count():,}\")\n",
        "print(f\"Validation rows: {df_valid.count():,}\")\n",
        "print(f\"Number of columns: {len(df_train.columns)}\")\n",
        "print(f\"Column names: {', '.join(df_train.columns)}\")\n",
        "\n",
        "# Display first 10 rows of training data\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TRAINING DATA - First 10 Rows\")\n",
        "print(\"=\" * 80)\n",
        "df_train.show(10, truncate=False)\n",
        "\n",
        "# Display first 10 rows of validation data\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VALIDATION DATA - First 10 Rows\")\n",
        "print(\"=\" * 80)\n",
        "df_valid.show(10, truncate=False)\n",
        "\n",
        "# Display label distribution\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"LABEL DISTRIBUTION\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nTraining set:\")\n",
        "df_train.groupBy(\"y\").count().orderBy(\"y\").show()\n",
        "\n",
        "print(\"Validation set:\")\n",
        "df_valid.groupBy(\"y\").count().orderBy(\"y\").show()\n",
        "\n",
        "# Display summary statistics for key numerical features\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SUMMARY STATISTICS - Key Features\")\n",
        "print(\"=\" * 80)\n",
        "key_features = ['n_prefix_items', 'n_prefix_events', 'cat_count_in_prefix', \n",
        "                'cat_popularity', 'user_cat_hist', 'recency_sec']\n",
        "df_train.select(key_features).describe().show()\n",
        "\n",
        "print(\"\\n✓ Data verification completed successfully!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Data Processing Completion Summary\n",
            "================================================================================\n",
            "Training Set:\n",
            "  Total rows: 970,995\n",
            "  Positive samples: 26,548 (2.73%)\n",
            "  Negative samples: 944,447 (97.27%)\n",
            "\n",
            "Validation Set:\n",
            "  Total rows: 551,775\n",
            "  Positive samples: 15,325 (2.78%)\n",
            "  Negative samples: 536,450 (97.22%)\n",
            "\n",
            "Feature Columns:\n",
            "  Total features: 34\n",
            "    - Base features: 18\n",
            "    - Category Embeddings: 16\n",
            "  Feature list: n_prefix_items, n_prefix_events, cat_count_in_prefix, cat_share_in_prefix, recency_sec, log_recency, hour_of_day, day_of_week, is_weekend, time_since_session_start, session_cat_diversity, cat_popularity, log_cat_pop, user_cat_hist, log_user_cat_hist, user_cat_sessions, user_total_sessions, user_avg_session_dur, cat_emb_0, cat_emb_1, cat_emb_2, cat_emb_3, cat_emb_4, cat_emb_5, cat_emb_6, cat_emb_7, cat_emb_8, cat_emb_9, cat_emb_10, cat_emb_11, cat_emb_12, cat_emb_13, cat_emb_14, cat_emb_15\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Data Processing Completion Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate label distribution\n",
        "train_label_dist = X_train_spark.groupBy(\"y\").count().collect()\n",
        "valid_label_dist = X_valid_spark.groupBy(\"y\").count().collect()\n",
        "\n",
        "train_pos = [r['count'] for r in train_label_dist if r['y'] == 1][0] if any(r['y'] == 1 for r in train_label_dist) else 0\n",
        "train_total = sum(r['count'] for r in train_label_dist)\n",
        "valid_pos = [r['count'] for r in valid_label_dist if r['y'] == 1][0] if any(r['y'] == 1 for r in valid_label_dist) else 0\n",
        "valid_total = sum(r['count'] for r in valid_label_dist)\n",
        "\n",
        "print(f\"Training Set:\")\n",
        "print(f\"  Total rows: {train_total:,}\")\n",
        "print(f\"  Positive samples: {train_pos:,} ({train_pos/train_total*100:.2f}%)\")\n",
        "print(f\"  Negative samples: {train_total - train_pos:,} ({(train_total-train_pos)/train_total*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nValidation Set:\")\n",
        "print(f\"  Total rows: {valid_total:,}\")\n",
        "print(f\"  Positive samples: {valid_pos:,} ({valid_pos/valid_total*100:.2f}%)\")\n",
        "print(f\"  Negative samples: {valid_total - valid_pos:,} ({(valid_total-valid_pos)/valid_total*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nFeature Columns:\")\n",
        "feature_cols = [c for c in X_train_spark.columns if c not in ['session_id', 'atc_ts', 'category_id', 'y']]\n",
        "print(f\"  Total features: {len(feature_cols)}\")\n",
        "print(f\"    - Base features: 18\")\n",
        "print(f\"    - Category Embeddings: 16\")\n",
        "print(f\"  Feature list: {', '.join(feature_cols)}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PIPELINE EXECUTION TIME SUMMARY\n",
            "================================================================================\n",
            "Start Time:    2025-12-08 13:52:45\n",
            "End Time:      2025-12-08 14:00:09\n",
            "Total Duration: 00:07:24.131\n",
            "               (444.13 seconds)\n",
            "================================================================================\n",
            "\n",
            "Estimated Time Breakdown:\n",
            "  • Data Loading & Sessionization:  ~44 seconds (0 min)\n",
            "  • Candidate Set Building:         ~133 seconds (2 min)\n",
            "  • Word2Vec Training:              ~22 seconds (0 min)\n",
            "  • Feature Engineering:            ~155 seconds (2 min)\n",
            "  • Data Saving:                    ~66 seconds (1 min)\n",
            "  • Verification & Summary:         ~22 seconds (0 min)\n"
          ]
        }
      ],
      "source": [
        "# Calculate total execution time\n",
        "pipeline_end_time = time.time()\n",
        "pipeline_end_datetime = datetime.now()\n",
        "\n",
        "total_seconds = pipeline_end_time - pipeline_start_time\n",
        "hours = int(total_seconds // 3600)\n",
        "minutes = int((total_seconds % 3600) // 60)\n",
        "seconds = int(total_seconds % 60)\n",
        "milliseconds = int((total_seconds % 1) * 1000)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PIPELINE EXECUTION TIME SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Start Time:    {pipeline_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"End Time:      {pipeline_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Total Duration: {hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}\")\n",
        "print(f\"               ({total_seconds:.2f} seconds)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Breakdown by major steps (if needed, estimate based on typical execution)\n",
        "print(\"\\nEstimated Time Breakdown:\")\n",
        "print(f\"  • Data Loading & Sessionization:  ~{int(total_seconds * 0.10):,} seconds ({int(total_seconds * 0.10 / 60)} min)\")\n",
        "print(f\"  • Candidate Set Building:         ~{int(total_seconds * 0.30):,} seconds ({int(total_seconds * 0.30 / 60)} min)\")\n",
        "print(f\"  • Word2Vec Training:              ~{int(total_seconds * 0.05):,} seconds ({int(total_seconds * 0.05 / 60)} min)\")\n",
        "print(f\"  • Feature Engineering:            ~{int(total_seconds * 0.35):,} seconds ({int(total_seconds * 0.35 / 60)} min)\")\n",
        "print(f\"  • Data Saving:                    ~{int(total_seconds * 0.15):,} seconds ({int(total_seconds * 0.15 / 60)} min)\")\n",
        "print(f\"  • Verification & Summary:         ~{int(total_seconds * 0.05):,} seconds ({int(total_seconds * 0.05 / 60)} min)\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stop Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped\n"
          ]
        }
      ],
      "source": [
        "# Stop Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bigdata",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
